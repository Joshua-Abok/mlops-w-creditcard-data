{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n"
     ]
    }
   ],
   "source": [
    "import pyspark # \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression as LogisticRegressionPySpark\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "import os \n",
    "import seaborn as sns \n",
    "import sklearn # \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import matplotlib # \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = '127.0.0.1'\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext._conf.getAll()\n",
    "\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark: 3.0.0\n",
      "matplotlib: 3.2.1\n",
      "seaborn: 0.10.1\n",
      "sklearn: 0.22.1\n",
      "mlflow: 1.23.1\n"
     ]
    }
   ],
   "source": [
    "print(\"pyspark: {}\".format(pyspark.__version__))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"seaborn: {}\".format(sns.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"mlflow: {}\".format(mlflow.__version__))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark: you must define a Spark context and a create a `Spark Session`.\n",
    "\n",
    "It means that you are creating a point of connection to the `Spark engine`, enabling the engine to run all of the code related to Spark functionality. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data set\n",
    "* PySpark has its own functionality for creating data frames. `No need to use Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"D:\\per\\Codes\\cloud_deployment\\mlops\\data\\creditcard.csv\\creditcard.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "labelColumn = \"Class\"\n",
    "columns = df.columns\n",
    "numericCols = columns\n",
    "numericCols.remove(\"Time\")\n",
    "numericCols.remove(labelColumn)\n",
    "print(numericCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+-------------------+------+-----+\n",
      "|Time|              V1|                 V2|              V3|               V4|                V5|                 V6|                 V7|                V8|                V9|               V10|               V11|               V12|               V13|               V14|              V15|               V16|               V17|               V18|               V19|                V20|               V21|               V22|               V23|               V24|              V25|               V26|                 V27|                V28|Amount|Class|\n",
      "+----+----------------+-------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+-------------------+------+-----+\n",
      "| 0.0|-1.3598071336738|-0.0727811733098497|2.53634673796914| 1.37815522427443|-0.338320769942518|  0.462387777762292|  0.239598554061257|0.0986979012610507| 0.363786969611213|0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879| 1.46817697209427|-0.470400525259478| 0.207971241929242|0.0257905801985591| 0.403992960255733|  0.251412098239705|-0.018306777944153| 0.277837575558899|-0.110473910188767|0.0669280749146731|0.128539358273528|-0.189114843888824|   0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "| 0.0|1.19185711131486|   0.26615071205963|0.16648011335321|0.448154078460911|0.0600176492822243|-0.0823608088155687|-0.0788029833323113|0.0851016549148104|-0.255425128109186|-0.166974414004614|  1.61272666105479|  1.06523531137287|  0.48909501589608|-0.143772296441519|0.635558093258208| 0.463917041022171|-0.114804663102346|-0.183361270123994|-0.145783041325259|-0.0690831352230203|-0.225775248033138|-0.638671952771851| 0.101288021253234|-0.339846475529127|0.167170404418143| 0.125894532368176|-0.00898309914322813| 0.0147241691924927|  2.69|    0|\n",
      "+----+----------------+-------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+-------------------+------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:88: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 0.15.1 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert PySpark data frames into pandas data frames \n",
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `PySpark's built-in` functionality to convert the spark data frame into pandas data frame for easier viewing\n",
    "* If a data frame has a lot of columns it'd be great to convert the `Spark data frame` into `Pandas` for viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, you would need to pass in a vector called `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "assemblerInputs = numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "dfFeatures = df.select(F.col(labelColumn).alias('label'), *numericCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* defines the inputs to the assembler so that it knows what columns to transform into the features vector\n",
    "* also constructing the `VectorAssembler` that will be used later to create a feature vector from the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, V1: double, V2: double, V3: double, V4: double, V5: double, V6: double, V7: double, V8: double, V9: double, V10: double, V11: double, V12: double, V13: double, V14: double, V15: double, V16: double, V17: double, V18: double, V19: double, V20: double, V21: double, V22: double, V23: double, V24: double, V25: double, V26: double, V27: double, V28: double, Amount: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add cell to the above and create the normal and anomaly data splits \n",
    "normal = dfFeatures.filter(\"Class == 0\").sample(withReplacement=False, fraction=0.5, seed=2020)\n",
    "anomaly = dfFeatures.filter(\"Class == 1\")\n",
    "\n",
    "normal_train, normal_test = normal.randomSplit([0.8, 0.2], seed=2020)\n",
    "anomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2], seed=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normal and anomaly data split similar to scikit-learn's, and split it into training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:88: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 0.15.1 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2      0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3      0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4      0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0  0.239599  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFeatures.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `combine the respective normal & anomaly splits` to form your training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No validation set, looking at an 80-20 split btn the training and testing data\n",
    "# creating the training and testing sets with PySpark's functionality\n",
    "train_set = normal_train.union(anomaly_train)\n",
    "test_set = normal_test.union(anomaly_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the pipeline with the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count:  113995\n",
      "Test Dataset Count:  28569\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(dfFeatures)\n",
    "train_set = pipelineModel.transform(train_set)\n",
    "test_set = pipelineModel.transform(test_set)\n",
    "\n",
    "selectedCols = ['label', 'features'] + numericCols\n",
    "train_set = train_set.select(selectedCols)\n",
    "test_set = test_set.select(selectedCols)\n",
    "\n",
    "print(\"Training Dataset Count: \", train_set.count())\n",
    "print(\"Test Dataset Count: \", test_set.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* created a `feature vector` from the data frame using the pipeline. Now, train `logistic regression model` on the feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(spark_model, train_set):\n",
    "\n",
    "    trained_model = spark_model.fit(train_set)\n",
    "\n",
    "    trainingSummary = trained_model.summary\n",
    "    pyspark_auc_score = trainingSummary.areaUnderROC\n",
    "\n",
    "    mlflow.log_metric(\"train_acc\", trainingSummary.accuracy)\n",
    "    mlflow.log_metric(\"train_AUC\", pyspark_auc_score)\n",
    "\n",
    "    print(\"Training Accuracy: \", trainingSummary.accuracy)\n",
    "    print(\"Training AUC: \", pyspark_auc_score)\n",
    "\n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defining the PySpark logistic regression model and log the training accuracy and the AUC score metrics  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = lrModel.transform(test)\n",
    "\n",
    "# y_true = predictions.select(['label']).collect()\n",
    "# y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "# evaluations = lrModel.evaluate(test)\n",
    "# accuracy = evaluations.accuracy\n",
    "\n",
    "def evaluate(spark_model, test_set):\n",
    "    evaluation_summary = spark_model.evaluate(test_set)\n",
    "\n",
    "    eval_acc = evaluation_summary.accuracy\n",
    "    eval_AUC = evaluation_summary.areaUnderROC\n",
    "\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"eval_AUC\", eval_AUC)\n",
    "\n",
    "    print(\"Evaluation Accuracy: \", eval_acc)\n",
    "    print(\"Evaluation AUC: \", eval_AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluates the trained PySpark logistic regression model model and log the evaluation accuracy and AUC Score metrics.\n",
    "* `AUC Score` is calculated using `scikit-learn's` scoring algorithm, while the `PySpark AUC Score metric` comes from the training summary of the `PySpark model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow Run - Training, UI, and Loading an MLFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = train(lr, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_a42cf0f54583, numClasses=2, numFeatures=29"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9987981929031975\n",
      "Training AUC:  0.9805692880720026\n",
      "Evaluation Accuracy:  0.9991249256186776\n",
      "Evaluation AUC:  0.9677957524292414\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o329.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:176)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:171)\r\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 314, kubernetes.docker.internal, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 D:\\tmp\\mlflow\\28418ffb-4a7b-4470-90e2-e6de31658dc6\\metadata\\_temporary\\0\\_temporary\\attempt_20230719230700_0211_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2146)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 67 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 D:\\tmp\\mlflow\\28418ffb-4a7b-4470-90e2-e6de31658dc6\\metadata\\_temporary\\0\\_temporary\\attempt_20230719230700_0211_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0a9210211a37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainedLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainedLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"creditcard_model_pyspark\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\mlflow\\spark.py\u001b[0m in \u001b[0;36mlog_model\u001b[1;34m(spark_model, artifact_path, conda_env, dfs_tmpdir, sample_input, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mawait_registration_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mawait_registration_for\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mpip_requirements\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpip_requirements\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[0mextra_pip_requirements\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_pip_requirements\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         )\n\u001b[0;32m    223\u001b[0m     \u001b[0mmodel_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_root_artifact_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martifact_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\mlflow\\models\\model.py\u001b[0m in \u001b[0;36mlog\u001b[1;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mrun_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfluent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_or_start_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0mmlflow_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[0mflavor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlflow_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmlflow_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m             \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfluent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_artifacts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martifact_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\mlflow\\spark.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(spark_model, path, mlflow_model, conda_env, dfs_tmpdir, sample_input, signature, input_example, pip_requirements, extra_pip_requirements)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0mdfs_tmpdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDFS_TMP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[0mtmp_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tmp_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs_tmpdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m     \u001b[0mspark_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m     \u001b[0msparkml_data_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SPARK_MODEL_PATH_SUB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[1;31m# We're copying the Spark model from DBFS to the local filesystem if (a) the temporary DFS URI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o329.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:176)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:171)\r\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 314, kubernetes.docker.internal, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 D:\\tmp\\mlflow\\28418ffb-4a7b-4470-90e2-e6de31658dc6\\metadata\\_temporary\\0\\_temporary\\attempt_20230719230700_0211_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2146)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 67 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 D:\\tmp\\mlflow\\28418ffb-4a7b-4470-90e2-e6de31658dc6\\metadata\\_temporary\\0\\_temporary\\attempt_20230719230700_0211_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# start MLFlow run and build a model\n",
    "lr = LogisticRegressionPySpark(featuresCol='features', labelCol='label', maxIter=10)\n",
    "\n",
    "mlflow.set_experiment(\"PySpark_CreditCard\")  # create an experiment\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trainedLR = train(lr, train_set)\n",
    "\n",
    "    evaluate(trainedLR, test_set)\n",
    "\n",
    "    mlflow.spark.log_model(trainedLR, \"creditcard_model_pyspark\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20f738172b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#examine the scores on a graph\n",
    "pyspark_roc = trainingSummary.roc.toPandas()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('PySpark ROC Curve')\n",
    "plt.plot(pyspark_roc['FPR'], pyspark_roc['TPR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ROC Curve for the *PySpark logistic regression model*\n",
    "> A perfect ROC Curve would have the *true positive rate* starting at *1.0*, where it continues right to a *false positive rate* value of 1.0. \n",
    "* This curve is quite close to that, hence why its area (AUC) is said to be around *0.97997 by PySpark*, keeping in mind a <a>perfect AUC Score</a> is *1.00*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 15.0, 'predicted')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the confusion matrix to check the model performance\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "ax = sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the accuracy of the correctly labeled points for the normal data is very high and is decent for the anomalous data\n",
    "* anomalies -> the model has a 75.86% accuracy (66/(66+21)) in predicting anomalies\n",
    "* normal points -> good accuracy of 99.98% (28475/(28475+7))\n",
    "* haven't tuned the hyperparameter to attain maximum performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
