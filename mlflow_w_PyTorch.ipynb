{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python36\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import mlflow \n",
    "import mlflow.pytorch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.6.0\n",
      "torchvision: 0.7.0\n",
      "sklearn: 0.22.1\n",
      "MLFlow: 1.23.1\n",
      "Numpy: 1.19.5\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch: {}\".format(torch.__version__))\n",
    "print(\"torchvision: {}\".format(torchvision.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"Device: \", device)   # tells PyTorch which device to run the code on\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic hyperparameters\n",
    "\n",
    "batch_size = 256\n",
    "num_classes = 10 \n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:21, 471491.91it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 36172.59it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:04, 391775.53it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 12700.57it/s]            \n",
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load the MNIST dataset - included as example dataset\n",
    "# defining the training and testing sets by loading the data from PyTorch\n",
    "train_set = torchvision.datasets.MNIST(root='./data', \n",
    "                                       train=True, download=True, transform=None)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', \n",
    "                                      train=False, download=True, transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x_train, y_train, x_test, and y_test datasets from the training & testing sets \n",
    "\n",
    "x_train, y_train = train_set.data, train_set.targets\n",
    "x_test, y_test = test_set.data, test_set.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'd want the data to be in channels first. `format(m, c, h, w)` \n",
    "* `m` -> number of samples\n",
    "* `w` -> width of the samples   \n",
    "`opposite` format of how Keras and TF2.0 would have\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([60000, 28, 28]) \n",
      " x_test shape: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train shape: {x_train.shape} \\n x_test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([60000, 1, 28, 28]) \n",
      " x_test shape: torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train shape: {x_train.shape} \\n x_test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reshaping the x-sets to encode the data in a `channels-first format (1 is the channel)` which is different to keras/tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]   # output of first sample in the y_train set - not in one-hot encoded format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* outputs a number not a vector. In keras/tf we were using `keras.utils.to_categorical()`. here will have to create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(num_classes, labels): \n",
    "    one_hot = torch.zeros(([labels.shape[0], num_classes]))\n",
    "    for f in range(len(labels)): \n",
    "        one_hot[f][labels[f]] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A custom function that converts the input called `labels`, given the number of classes, into a one-hot encoded format and returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert your y-sets into one-hot encoded format\n",
    "\n",
    "y_train = to_one_hot(num_classes, y_train)\n",
    "y_test = to_one_hot(num_classes, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]  # tensor now converted into one-hot encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x_train: torch.Size([60000, 1, 28, 28])\n",
      "y_train: torch.Size([60000, 10])\n",
      "x_test: torch.Size([10000, 1, 28, 28])\n",
      "y_test: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes\")\n",
    "print(\"x_train: {}\\ny_train: {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test: {}\\ny_test: {}\".format(x_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x-sets in `channels-first` format and y-sets in `one-hot encoded` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow Run - Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model architecture as a class\n",
    "\n",
    "class model(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        # IN 1x28x28 OUT 16x14x14\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, \n",
    "                               padding=1, dilation=1)\n",
    "        # IN 16x14x14 OUT 32x6x6\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, \n",
    "                               padding=0, dilation=1)\n",
    "        # IN 32x6x6 OUT 64x2x2\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, \n",
    "                               padding=0, dilation=1)\n",
    "         # IN 64x2x2 OUT 256\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(in_features=256, \n",
    "                                out_features=128)\n",
    "        self.dense2 = nn.Linear(in_features=128, \n",
    "                                out_features=64)\n",
    "        self.dense3 = nn.Linear(in_features=64, \n",
    "                                out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       x = self.conv1(x)\n",
    "       x = nn.ReLU()(x)\n",
    "       x = self.conv2(x)\n",
    "       x = nn.ReLU()(x)\n",
    "       x = self.conv3(x)\n",
    "       x = nn.ReLU()(x)\n",
    "       x = self.flat1(x)\n",
    "       x = self.dense1(x)\n",
    "       x = nn.ReLU()(x)\n",
    "       x = self.dense2(x)\n",
    "       x = nn.ReLU()(x)\n",
    "       x = self.dense3(x)\n",
    "       x = nn.Softmax()(x)\n",
    "       return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the model to the device\n",
    "model = model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)     # defining an adam optimizer with lr\n",
    "criterion = nn.BCELoss()  # initializing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a data loader to take care of batching your data set\n",
    "dataset = data.TensorDataset(x_train, y_train)\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creating a data loader object out of your data set -> PyTorch batches your data set for you, allowing you to pass in a `minibatch at a time` in your training loop. This essentially is what the tf2.0/Keras.fit() function does, but it's all abstracted for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch_Num 0 Loss 0.3306376039981842\n",
      "Epoch 0 Batch_Num 1 Loss 0.3082179129123688\n",
      "Epoch 0 Batch_Num 2 Loss 0.29878902435302734\n",
      "Epoch 0 Batch_Num 3 Loss 0.2863521873950958\n",
      "Epoch 0 Batch_Num 4 Loss 0.27009230852127075\n",
      "Epoch 0 Batch_Num 5 Loss 0.24328410625457764\n",
      "Epoch 0 Batch_Num 6 Loss 0.21504172682762146\n",
      "Epoch 0 Batch_Num 7 Loss 0.18875226378440857\n",
      "Epoch 0 Batch_Num 8 Loss 0.16653653979301453\n",
      "Epoch 0 Batch_Num 9 Loss 0.15639765560626984\n",
      "Epoch 0 Batch_Num 10 Loss 0.1510590761899948\n",
      "Epoch 0 Batch_Num 11 Loss 0.1324325054883957\n",
      "Epoch 0 Batch_Num 12 Loss 0.1265551894903183\n",
      "Epoch 0 Batch_Num 13 Loss 0.12236194312572479\n",
      "Epoch 0 Batch_Num 14 Loss 0.12856902182102203\n",
      "Epoch 0 Batch_Num 15 Loss 0.12779393792152405\n",
      "Epoch 0 Batch_Num 16 Loss 0.10219927132129669\n",
      "Epoch 0 Batch_Num 17 Loss 0.09441255033016205\n",
      "Epoch 0 Batch_Num 18 Loss 0.11018530279397964\n",
      "Epoch 0 Batch_Num 19 Loss 0.09857948869466782\n",
      "Epoch 0 Batch_Num 20 Loss 0.09449602663516998\n",
      "Epoch 0 Batch_Num 21 Loss 0.07608035951852798\n",
      "Epoch 0 Batch_Num 22 Loss 0.10760907828807831\n",
      "Epoch 0 Batch_Num 23 Loss 0.06776659190654755\n",
      "Epoch 0 Batch_Num 24 Loss 0.05689782649278641\n",
      "Epoch 0 Batch_Num 25 Loss 0.05879148840904236\n",
      "Epoch 0 Batch_Num 26 Loss 0.09015966206789017\n",
      "Epoch 0 Batch_Num 27 Loss 0.11029300838708878\n",
      "Epoch 0 Batch_Num 28 Loss 0.1070999726653099\n",
      "Epoch 0 Batch_Num 29 Loss 0.07479254901409149\n",
      "Epoch 0 Batch_Num 30 Loss 0.09509967267513275\n",
      "Epoch 0 Batch_Num 31 Loss 0.07519275695085526\n",
      "Epoch 0 Batch_Num 32 Loss 0.08559258282184601\n",
      "Epoch 0 Batch_Num 33 Loss 0.09035446494817734\n",
      "Epoch 0 Batch_Num 34 Loss 0.1117192879319191\n",
      "Epoch 0 Batch_Num 35 Loss 0.05788782984018326\n",
      "Epoch 0 Batch_Num 36 Loss 0.09418560564517975\n",
      "Epoch 0 Batch_Num 37 Loss 0.0533800832927227\n",
      "Epoch 0 Batch_Num 38 Loss 0.06479077786207199\n",
      "Epoch 0 Batch_Num 39 Loss 0.06690169870853424\n",
      "Epoch 0 Batch_Num 40 Loss 0.06561974436044693\n",
      "Epoch 0 Batch_Num 41 Loss 0.058063358068466187\n",
      "Epoch 0 Batch_Num 42 Loss 0.05970185995101929\n",
      "Epoch 0 Batch_Num 43 Loss 0.05363354831933975\n",
      "Epoch 0 Batch_Num 44 Loss 0.052140913903713226\n",
      "Epoch 0 Batch_Num 45 Loss 0.09806513786315918\n",
      "Epoch 0 Batch_Num 46 Loss 0.05443103238940239\n",
      "Epoch 0 Batch_Num 47 Loss 0.055936433374881744\n",
      "Epoch 0 Batch_Num 48 Loss 0.07693641632795334\n",
      "Epoch 0 Batch_Num 49 Loss 0.09855382889509201\n",
      "Epoch 0 Batch_Num 50 Loss 0.07745404541492462\n",
      "Epoch 0 Batch_Num 51 Loss 0.06433264911174774\n",
      "Epoch 0 Batch_Num 52 Loss 0.04780708998441696\n",
      "Epoch 0 Batch_Num 53 Loss 0.04454384744167328\n",
      "Epoch 0 Batch_Num 54 Loss 0.0659138411283493\n",
      "Epoch 0 Batch_Num 55 Loss 0.06108816713094711\n",
      "Epoch 0 Batch_Num 56 Loss 0.07105304300785065\n",
      "Epoch 0 Batch_Num 57 Loss 0.08130968362092972\n",
      "Epoch 0 Batch_Num 58 Loss 0.04266515374183655\n",
      "Epoch 0 Batch_Num 59 Loss 0.05887598544359207\n",
      "Epoch 0 Batch_Num 60 Loss 0.04428604990243912\n",
      "Epoch 0 Batch_Num 61 Loss 0.058701444417238235\n",
      "Epoch 0 Batch_Num 62 Loss 0.06482572108507156\n",
      "Epoch 0 Batch_Num 63 Loss 0.03285165876150131\n",
      "Epoch 0 Batch_Num 64 Loss 0.03681985288858414\n",
      "Epoch 0 Batch_Num 65 Loss 0.051117487251758575\n",
      "Epoch 0 Batch_Num 66 Loss 0.05573881417512894\n",
      "Epoch 0 Batch_Num 67 Loss 0.03575760871171951\n",
      "Epoch 0 Batch_Num 68 Loss 0.05376895144581795\n",
      "Epoch 0 Batch_Num 69 Loss 0.05572833493351936\n",
      "Epoch 0 Batch_Num 70 Loss 0.025572339072823524\n",
      "Epoch 0 Batch_Num 71 Loss 0.038224004209041595\n",
      "Epoch 0 Batch_Num 72 Loss 0.03924902155995369\n",
      "Epoch 0 Batch_Num 73 Loss 0.039464324712753296\n",
      "Epoch 0 Batch_Num 74 Loss 0.04301730915904045\n",
      "Epoch 0 Batch_Num 75 Loss 0.044175226241350174\n",
      "Epoch 0 Batch_Num 76 Loss 0.04765550047159195\n",
      "Epoch 0 Batch_Num 77 Loss 0.03328721970319748\n",
      "Epoch 0 Batch_Num 78 Loss 0.07614241540431976\n",
      "Epoch 0 Batch_Num 79 Loss 0.041898488998413086\n",
      "Epoch 0 Batch_Num 80 Loss 0.047199465334415436\n",
      "Epoch 0 Batch_Num 81 Loss 0.06808989495038986\n",
      "Epoch 0 Batch_Num 82 Loss 0.032889701426029205\n",
      "Epoch 0 Batch_Num 83 Loss 0.02827572263777256\n",
      "Epoch 0 Batch_Num 84 Loss 0.04171769693493843\n",
      "Epoch 0 Batch_Num 85 Loss 0.024701310321688652\n",
      "Epoch 0 Batch_Num 86 Loss 0.05429043620824814\n",
      "Epoch 0 Batch_Num 87 Loss 0.03922979161143303\n",
      "Epoch 0 Batch_Num 88 Loss 0.03915369510650635\n",
      "Epoch 0 Batch_Num 89 Loss 0.02900077775120735\n",
      "Epoch 0 Batch_Num 90 Loss 0.031075870618224144\n",
      "Epoch 0 Batch_Num 91 Loss 0.03151216357946396\n",
      "Epoch 0 Batch_Num 92 Loss 0.03719376027584076\n",
      "Epoch 0 Batch_Num 93 Loss 0.03431010991334915\n",
      "Epoch 0 Batch_Num 94 Loss 0.04240492358803749\n",
      "Epoch 0 Batch_Num 95 Loss 0.03892653062939644\n",
      "Epoch 0 Batch_Num 96 Loss 0.058145564049482346\n",
      "Epoch 0 Batch_Num 97 Loss 0.027609091252088547\n",
      "Epoch 0 Batch_Num 98 Loss 0.03249259665608406\n",
      "Epoch 0 Batch_Num 99 Loss 0.017106134444475174\n",
      "Epoch 0 Batch_Num 100 Loss 0.02716689184308052\n",
      "Epoch 0 Batch_Num 101 Loss 0.02116333693265915\n",
      "Epoch 0 Batch_Num 102 Loss 0.023767227306962013\n",
      "Epoch 0 Batch_Num 103 Loss 0.04775511845946312\n",
      "Epoch 0 Batch_Num 104 Loss 0.05593014508485794\n",
      "Epoch 0 Batch_Num 105 Loss 0.03183319792151451\n",
      "Epoch 0 Batch_Num 106 Loss 0.03811143711209297\n",
      "Epoch 0 Batch_Num 107 Loss 0.02685919776558876\n",
      "Epoch 0 Batch_Num 108 Loss 0.035361576825380325\n",
      "Epoch 0 Batch_Num 109 Loss 0.025911837816238403\n",
      "Epoch 0 Batch_Num 110 Loss 0.0437900610268116\n",
      "Epoch 0 Batch_Num 111 Loss 0.03431969881057739\n",
      "Epoch 0 Batch_Num 112 Loss 0.02548094093799591\n",
      "Epoch 0 Batch_Num 113 Loss 0.029840975999832153\n",
      "Epoch 0 Batch_Num 114 Loss 0.03942374140024185\n",
      "Epoch 0 Batch_Num 115 Loss 0.02436853013932705\n",
      "Epoch 0 Batch_Num 116 Loss 0.03698042035102844\n",
      "Epoch 0 Batch_Num 117 Loss 0.03828439116477966\n",
      "Epoch 0 Batch_Num 118 Loss 0.019260786473751068\n",
      "Epoch 0 Batch_Num 119 Loss 0.0444493405520916\n",
      "Epoch 0 Batch_Num 120 Loss 0.02919366955757141\n",
      "Epoch 0 Batch_Num 121 Loss 0.022418301552534103\n",
      "Epoch 0 Batch_Num 122 Loss 0.04524940997362137\n",
      "Epoch 0 Batch_Num 123 Loss 0.0351981446146965\n",
      "Epoch 0 Batch_Num 124 Loss 0.026087243109941483\n",
      "Epoch 0 Batch_Num 125 Loss 0.02682124637067318\n",
      "Epoch 0 Batch_Num 126 Loss 0.05217193812131882\n",
      "Epoch 0 Batch_Num 127 Loss 0.018660906702280045\n",
      "Epoch 0 Batch_Num 128 Loss 0.025780346244573593\n",
      "Epoch 0 Batch_Num 129 Loss 0.02512286603450775\n",
      "Epoch 0 Batch_Num 130 Loss 0.029892150312662125\n",
      "Epoch 0 Batch_Num 131 Loss 0.027928773313760757\n",
      "Epoch 0 Batch_Num 132 Loss 0.014685170724987984\n",
      "Epoch 0 Batch_Num 133 Loss 0.013338935561478138\n",
      "Epoch 0 Batch_Num 134 Loss 0.03658631071448326\n",
      "Epoch 0 Batch_Num 135 Loss 0.028092050924897194\n",
      "Epoch 0 Batch_Num 136 Loss 0.025909459218382835\n",
      "Epoch 0 Batch_Num 137 Loss 0.02522064372897148\n",
      "Epoch 0 Batch_Num 138 Loss 0.02382947877049446\n",
      "Epoch 0 Batch_Num 139 Loss 0.01943805254995823\n",
      "Epoch 0 Batch_Num 140 Loss 0.031341128051280975\n",
      "Epoch 0 Batch_Num 141 Loss 0.020345456898212433\n",
      "Epoch 0 Batch_Num 142 Loss 0.025389278307557106\n",
      "Epoch 0 Batch_Num 143 Loss 0.01966063305735588\n",
      "Epoch 0 Batch_Num 144 Loss 0.02543288841843605\n",
      "Epoch 0 Batch_Num 145 Loss 0.02811155654489994\n",
      "Epoch 0 Batch_Num 146 Loss 0.034235529601573944\n",
      "Epoch 0 Batch_Num 147 Loss 0.035270094871520996\n",
      "Epoch 0 Batch_Num 148 Loss 0.02231258526444435\n",
      "Epoch 0 Batch_Num 149 Loss 0.026050101965665817\n",
      "Epoch 0 Batch_Num 150 Loss 0.027821402996778488\n",
      "Epoch 0 Batch_Num 151 Loss 0.01616513729095459\n",
      "Epoch 0 Batch_Num 152 Loss 0.012896913103759289\n",
      "Epoch 0 Batch_Num 153 Loss 0.04250433295965195\n",
      "Epoch 0 Batch_Num 154 Loss 0.03104720078408718\n",
      "Epoch 0 Batch_Num 155 Loss 0.02938566729426384\n",
      "Epoch 0 Batch_Num 156 Loss 0.02249581553041935\n",
      "Epoch 0 Batch_Num 157 Loss 0.02447604201734066\n",
      "Epoch 0 Batch_Num 158 Loss 0.01638137921690941\n",
      "Epoch 0 Batch_Num 159 Loss 0.017681485041975975\n",
      "Epoch 0 Batch_Num 160 Loss 0.021679440513253212\n",
      "Epoch 0 Batch_Num 161 Loss 0.043577782809734344\n",
      "Epoch 0 Batch_Num 162 Loss 0.02403029426932335\n",
      "Epoch 0 Batch_Num 163 Loss 0.023884862661361694\n",
      "Epoch 0 Batch_Num 164 Loss 0.029667887836694717\n",
      "Epoch 0 Batch_Num 165 Loss 0.020934974774718285\n",
      "Epoch 0 Batch_Num 166 Loss 0.03019268810749054\n",
      "Epoch 0 Batch_Num 167 Loss 0.03216281160712242\n",
      "Epoch 0 Batch_Num 168 Loss 0.037568099796772\n",
      "Epoch 0 Batch_Num 169 Loss 0.014689376577734947\n",
      "Epoch 0 Batch_Num 170 Loss 0.019410040229558945\n",
      "Epoch 0 Batch_Num 171 Loss 0.029909977689385414\n",
      "Epoch 0 Batch_Num 172 Loss 0.03897298127412796\n",
      "Epoch 0 Batch_Num 173 Loss 0.03688737377524376\n",
      "Epoch 0 Batch_Num 174 Loss 0.016801411285996437\n",
      "Epoch 0 Batch_Num 175 Loss 0.030510077252984047\n",
      "Epoch 0 Batch_Num 176 Loss 0.01882786862552166\n",
      "Epoch 0 Batch_Num 177 Loss 0.021307338029146194\n",
      "Epoch 0 Batch_Num 178 Loss 0.03430579602718353\n",
      "Epoch 0 Batch_Num 179 Loss 0.033817108720541\n",
      "Epoch 0 Batch_Num 180 Loss 0.046757884323596954\n",
      "Epoch 0 Batch_Num 181 Loss 0.02698136493563652\n",
      "Epoch 0 Batch_Num 182 Loss 0.026758592575788498\n",
      "Epoch 0 Batch_Num 183 Loss 0.02314651384949684\n",
      "Epoch 0 Batch_Num 184 Loss 0.03273483365774155\n",
      "Epoch 0 Batch_Num 185 Loss 0.030444452539086342\n",
      "Epoch 0 Batch_Num 186 Loss 0.022816523909568787\n",
      "Epoch 0 Batch_Num 187 Loss 0.02393481694161892\n",
      "Epoch 0 Batch_Num 188 Loss 0.017208050936460495\n",
      "Epoch 0 Batch_Num 189 Loss 0.014032309874892235\n",
      "Epoch 0 Batch_Num 190 Loss 0.011442125774919987\n",
      "Epoch 0 Batch_Num 191 Loss 0.04262464493513107\n",
      "Epoch 0 Batch_Num 192 Loss 0.018757356330752373\n",
      "Epoch 0 Batch_Num 193 Loss 0.044261034578084946\n",
      "Epoch 0 Batch_Num 194 Loss 0.023593222722411156\n",
      "Epoch 0 Batch_Num 195 Loss 0.020000746473670006\n",
      "Epoch 0 Batch_Num 196 Loss 0.04860493913292885\n",
      "Epoch 0 Batch_Num 197 Loss 0.013327050022780895\n",
      "Epoch 0 Batch_Num 198 Loss 0.022218938916921616\n",
      "Epoch 0 Batch_Num 199 Loss 0.01254958100616932\n",
      "Epoch 0 Batch_Num 200 Loss 0.020252063870429993\n",
      "Epoch 0 Batch_Num 201 Loss 0.016566550359129906\n",
      "Epoch 0 Batch_Num 202 Loss 0.01417677104473114\n",
      "Epoch 0 Batch_Num 203 Loss 0.03687836974859238\n",
      "Epoch 0 Batch_Num 204 Loss 0.02207874320447445\n",
      "Epoch 0 Batch_Num 205 Loss 0.015161067247390747\n",
      "Epoch 0 Batch_Num 206 Loss 0.05406094342470169\n",
      "Epoch 0 Batch_Num 207 Loss 0.02152995951473713\n",
      "Epoch 0 Batch_Num 208 Loss 0.012873870320618153\n",
      "Epoch 0 Batch_Num 209 Loss 0.02734586037695408\n",
      "Epoch 0 Batch_Num 210 Loss 0.027088487520813942\n",
      "Epoch 0 Batch_Num 211 Loss 0.0129759032279253\n",
      "Epoch 0 Batch_Num 212 Loss 0.015382143668830395\n",
      "Epoch 0 Batch_Num 213 Loss 0.015239283442497253\n",
      "Epoch 0 Batch_Num 214 Loss 0.03626905754208565\n",
      "Epoch 0 Batch_Num 215 Loss 0.013946795836091042\n",
      "Epoch 0 Batch_Num 216 Loss 0.03210029378533363\n",
      "Epoch 0 Batch_Num 217 Loss 0.02401583269238472\n",
      "Epoch 0 Batch_Num 218 Loss 0.01425789762288332\n",
      "Epoch 0 Batch_Num 219 Loss 0.01810530759394169\n",
      "Epoch 0 Batch_Num 220 Loss 0.021914461627602577\n",
      "Epoch 0 Batch_Num 221 Loss 0.01766750030219555\n",
      "Epoch 0 Batch_Num 222 Loss 0.015167010016739368\n",
      "Epoch 0 Batch_Num 223 Loss 0.008944655768573284\n",
      "Epoch 0 Batch_Num 224 Loss 0.013210940174758434\n",
      "Epoch 0 Batch_Num 225 Loss 0.030949274078011513\n",
      "Epoch 0 Batch_Num 226 Loss 0.011334974318742752\n",
      "Epoch 0 Batch_Num 227 Loss 0.003744129091501236\n",
      "Epoch 0 Batch_Num 228 Loss 0.006962553597986698\n",
      "Epoch 0 Batch_Num 229 Loss 0.011752302758395672\n",
      "Epoch 0 Batch_Num 230 Loss 0.001491945586167276\n",
      "Epoch 0 Batch_Num 231 Loss 0.014566227793693542\n",
      "Epoch 0 Batch_Num 232 Loss 0.009483122266829014\n",
      "Epoch 0 Batch_Num 233 Loss 0.025883018970489502\n",
      "Epoch 0 Batch_Num 234 Loss 0.0284721739590168\n",
      "Epoch 1 Batch_Num 0 Loss 0.02946852520108223\n",
      "Epoch 1 Batch_Num 1 Loss 0.017239416018128395\n",
      "Epoch 1 Batch_Num 2 Loss 0.02043747901916504\n",
      "Epoch 1 Batch_Num 3 Loss 0.030565956607460976\n",
      "Epoch 1 Batch_Num 4 Loss 0.0247642919421196\n",
      "Epoch 1 Batch_Num 5 Loss 0.016857098788022995\n",
      "Epoch 1 Batch_Num 6 Loss 0.02245889976620674\n",
      "Epoch 1 Batch_Num 7 Loss 0.020194262266159058\n",
      "Epoch 1 Batch_Num 8 Loss 0.01376603078097105\n",
      "Epoch 1 Batch_Num 9 Loss 0.01665770635008812\n",
      "Epoch 1 Batch_Num 10 Loss 0.028718888759613037\n",
      "Epoch 1 Batch_Num 11 Loss 0.019961440935730934\n",
      "Epoch 1 Batch_Num 12 Loss 0.011232199147343636\n",
      "Epoch 1 Batch_Num 13 Loss 0.014210003428161144\n",
      "Epoch 1 Batch_Num 14 Loss 0.018446382135152817\n",
      "Epoch 1 Batch_Num 15 Loss 0.020318347960710526\n",
      "Epoch 1 Batch_Num 16 Loss 0.01805388554930687\n",
      "Epoch 1 Batch_Num 17 Loss 0.017973538488149643\n",
      "Epoch 1 Batch_Num 18 Loss 0.01780826784670353\n",
      "Epoch 1 Batch_Num 19 Loss 0.01827392727136612\n",
      "Epoch 1 Batch_Num 20 Loss 0.02283981442451477\n",
      "Epoch 1 Batch_Num 21 Loss 0.009420832619071007\n",
      "Epoch 1 Batch_Num 22 Loss 0.028250301256775856\n",
      "Epoch 1 Batch_Num 23 Loss 0.011652812361717224\n",
      "Epoch 1 Batch_Num 24 Loss 0.015631452202796936\n",
      "Epoch 1 Batch_Num 25 Loss 0.019597936421632767\n",
      "Epoch 1 Batch_Num 26 Loss 0.026684362441301346\n",
      "Epoch 1 Batch_Num 27 Loss 0.02435903623700142\n",
      "Epoch 1 Batch_Num 28 Loss 0.01935327798128128\n",
      "Epoch 1 Batch_Num 29 Loss 0.013712139800190926\n",
      "Epoch 1 Batch_Num 30 Loss 0.020924735814332962\n",
      "Epoch 1 Batch_Num 31 Loss 0.015386098995804787\n",
      "Epoch 1 Batch_Num 32 Loss 0.02210722304880619\n",
      "Epoch 1 Batch_Num 33 Loss 0.02328406274318695\n",
      "Epoch 1 Batch_Num 34 Loss 0.0422789566218853\n",
      "Epoch 1 Batch_Num 35 Loss 0.016259776428341866\n",
      "Epoch 1 Batch_Num 36 Loss 0.02825053595006466\n",
      "Epoch 1 Batch_Num 37 Loss 0.016245273873209953\n",
      "Epoch 1 Batch_Num 38 Loss 0.01067342609167099\n",
      "Epoch 1 Batch_Num 39 Loss 0.019607845693826675\n",
      "Epoch 1 Batch_Num 40 Loss 0.010473588481545448\n",
      "Epoch 1 Batch_Num 41 Loss 0.010538006201386452\n",
      "Epoch 1 Batch_Num 42 Loss 0.021005909889936447\n",
      "Epoch 1 Batch_Num 43 Loss 0.008229127153754234\n",
      "Epoch 1 Batch_Num 44 Loss 0.01478501595556736\n",
      "Epoch 1 Batch_Num 45 Loss 0.02665415033698082\n",
      "Epoch 1 Batch_Num 46 Loss 0.016739260405302048\n",
      "Epoch 1 Batch_Num 47 Loss 0.015354787930846214\n",
      "Epoch 1 Batch_Num 48 Loss 0.015237410552799702\n",
      "Epoch 1 Batch_Num 49 Loss 0.018264926970005035\n",
      "Epoch 1 Batch_Num 50 Loss 0.016779299825429916\n",
      "Epoch 1 Batch_Num 51 Loss 0.020247381180524826\n",
      "Epoch 1 Batch_Num 52 Loss 0.015329405665397644\n",
      "Epoch 1 Batch_Num 53 Loss 0.013045798055827618\n",
      "Epoch 1 Batch_Num 54 Loss 0.019433321431279182\n",
      "Epoch 1 Batch_Num 55 Loss 0.021951615810394287\n",
      "Epoch 1 Batch_Num 56 Loss 0.020451702177524567\n",
      "Epoch 1 Batch_Num 57 Loss 0.022066663950681686\n",
      "Epoch 1 Batch_Num 58 Loss 0.006332022603601217\n",
      "Epoch 1 Batch_Num 59 Loss 0.021116888150572777\n",
      "Epoch 1 Batch_Num 60 Loss 0.016267912462353706\n",
      "Epoch 1 Batch_Num 61 Loss 0.017005516216158867\n",
      "Epoch 1 Batch_Num 62 Loss 0.027331750839948654\n",
      "Epoch 1 Batch_Num 63 Loss 0.016868282109498978\n",
      "Epoch 1 Batch_Num 64 Loss 0.014417765662074089\n",
      "Epoch 1 Batch_Num 65 Loss 0.017952965572476387\n",
      "Epoch 1 Batch_Num 66 Loss 0.017036940902471542\n",
      "Epoch 1 Batch_Num 67 Loss 0.01839095912873745\n",
      "Epoch 1 Batch_Num 68 Loss 0.023724956437945366\n",
      "Epoch 1 Batch_Num 69 Loss 0.02525034174323082\n",
      "Epoch 1 Batch_Num 70 Loss 0.013682328164577484\n",
      "Epoch 1 Batch_Num 71 Loss 0.012162754312157631\n",
      "Epoch 1 Batch_Num 72 Loss 0.010757455602288246\n",
      "Epoch 1 Batch_Num 73 Loss 0.011760273948311806\n",
      "Epoch 1 Batch_Num 74 Loss 0.015711428597569466\n",
      "Epoch 1 Batch_Num 75 Loss 0.015499323606491089\n",
      "Epoch 1 Batch_Num 76 Loss 0.011128785088658333\n",
      "Epoch 1 Batch_Num 77 Loss 0.011553602293133736\n",
      "Epoch 1 Batch_Num 78 Loss 0.02469266578555107\n",
      "Epoch 1 Batch_Num 79 Loss 0.013346676714718342\n",
      "Epoch 1 Batch_Num 80 Loss 0.022174537181854248\n",
      "Epoch 1 Batch_Num 81 Loss 0.028551246970891953\n",
      "Epoch 1 Batch_Num 82 Loss 0.013049674220383167\n",
      "Epoch 1 Batch_Num 83 Loss 0.008120382204651833\n",
      "Epoch 1 Batch_Num 84 Loss 0.013598042540252209\n",
      "Epoch 1 Batch_Num 85 Loss 0.013124990276992321\n",
      "Epoch 1 Batch_Num 86 Loss 0.013454770669341087\n",
      "Epoch 1 Batch_Num 87 Loss 0.01495190430432558\n",
      "Epoch 1 Batch_Num 88 Loss 0.021518444642424583\n",
      "Epoch 1 Batch_Num 89 Loss 0.006923763547092676\n",
      "Epoch 1 Batch_Num 90 Loss 0.011324694380164146\n",
      "Epoch 1 Batch_Num 91 Loss 0.014962496235966682\n",
      "Epoch 1 Batch_Num 92 Loss 0.01788119412958622\n",
      "Epoch 1 Batch_Num 93 Loss 0.01735241524875164\n",
      "Epoch 1 Batch_Num 94 Loss 0.014755983836948872\n",
      "Epoch 1 Batch_Num 95 Loss 0.01142826210707426\n",
      "Epoch 1 Batch_Num 96 Loss 0.024506935849785805\n",
      "Epoch 1 Batch_Num 97 Loss 0.010060303844511509\n",
      "Epoch 1 Batch_Num 98 Loss 0.014040181413292885\n",
      "Epoch 1 Batch_Num 99 Loss 0.008104640059173107\n",
      "Epoch 1 Batch_Num 100 Loss 0.017521264031529427\n",
      "Epoch 1 Batch_Num 101 Loss 0.01056114211678505\n",
      "Epoch 1 Batch_Num 102 Loss 0.00620284304022789\n",
      "Epoch 1 Batch_Num 103 Loss 0.029525768011808395\n",
      "Epoch 1 Batch_Num 104 Loss 0.027013953775167465\n",
      "Epoch 1 Batch_Num 105 Loss 0.016954686492681503\n",
      "Epoch 1 Batch_Num 106 Loss 0.01348130963742733\n",
      "Epoch 1 Batch_Num 107 Loss 0.02065923810005188\n",
      "Epoch 1 Batch_Num 108 Loss 0.02360968478024006\n",
      "Epoch 1 Batch_Num 109 Loss 0.018537171185016632\n",
      "Epoch 1 Batch_Num 110 Loss 0.02097206935286522\n",
      "Epoch 1 Batch_Num 111 Loss 0.018974250182509422\n",
      "Epoch 1 Batch_Num 112 Loss 0.011670432984828949\n",
      "Epoch 1 Batch_Num 113 Loss 0.014856753870844841\n",
      "Epoch 1 Batch_Num 114 Loss 0.016016650944948196\n",
      "Epoch 1 Batch_Num 115 Loss 0.014618756249547005\n",
      "Epoch 1 Batch_Num 116 Loss 0.018931517377495766\n",
      "Epoch 1 Batch_Num 117 Loss 0.014865562319755554\n",
      "Epoch 1 Batch_Num 118 Loss 0.007153120823204517\n",
      "Epoch 1 Batch_Num 119 Loss 0.020612649619579315\n",
      "Epoch 1 Batch_Num 120 Loss 0.011628610081970692\n",
      "Epoch 1 Batch_Num 121 Loss 0.011749192140996456\n",
      "Epoch 1 Batch_Num 122 Loss 0.0211651474237442\n",
      "Epoch 1 Batch_Num 123 Loss 0.022327914834022522\n",
      "Epoch 1 Batch_Num 124 Loss 0.012003136798739433\n",
      "Epoch 1 Batch_Num 125 Loss 0.0106630465015769\n",
      "Epoch 1 Batch_Num 126 Loss 0.02334083989262581\n",
      "Epoch 1 Batch_Num 127 Loss 0.004171890206634998\n",
      "Epoch 1 Batch_Num 128 Loss 0.015499536879360676\n",
      "Epoch 1 Batch_Num 129 Loss 0.017827019095420837\n",
      "Epoch 1 Batch_Num 130 Loss 0.018318207934498787\n",
      "Epoch 1 Batch_Num 131 Loss 0.018481601029634476\n",
      "Epoch 1 Batch_Num 132 Loss 0.007433195598423481\n",
      "Epoch 1 Batch_Num 133 Loss 0.009882217273116112\n",
      "Epoch 1 Batch_Num 134 Loss 0.023679818958044052\n",
      "Epoch 1 Batch_Num 135 Loss 0.02053958922624588\n",
      "Epoch 1 Batch_Num 136 Loss 0.015544520691037178\n",
      "Epoch 1 Batch_Num 137 Loss 0.01215633936226368\n",
      "Epoch 1 Batch_Num 138 Loss 0.010417642071843147\n",
      "Epoch 1 Batch_Num 139 Loss 0.008668268099427223\n",
      "Epoch 1 Batch_Num 140 Loss 0.012215996161103249\n",
      "Epoch 1 Batch_Num 141 Loss 0.01184151042252779\n",
      "Epoch 1 Batch_Num 142 Loss 0.01408376358449459\n",
      "Epoch 1 Batch_Num 143 Loss 0.011151672340929508\n",
      "Epoch 1 Batch_Num 144 Loss 0.01577666401863098\n",
      "Epoch 1 Batch_Num 145 Loss 0.009266615845263004\n",
      "Epoch 1 Batch_Num 146 Loss 0.020454106852412224\n",
      "Epoch 1 Batch_Num 147 Loss 0.0244023185223341\n",
      "Epoch 1 Batch_Num 148 Loss 0.00846802070736885\n",
      "Epoch 1 Batch_Num 149 Loss 0.00784536637365818\n",
      "Epoch 1 Batch_Num 150 Loss 0.013141661882400513\n",
      "Epoch 1 Batch_Num 151 Loss 0.013542373664677143\n",
      "Epoch 1 Batch_Num 152 Loss 0.007404777221381664\n",
      "Epoch 1 Batch_Num 153 Loss 0.024959692731499672\n",
      "Epoch 1 Batch_Num 154 Loss 0.01895025745034218\n",
      "Epoch 1 Batch_Num 155 Loss 0.016518209129571915\n",
      "Epoch 1 Batch_Num 156 Loss 0.015134068205952644\n",
      "Epoch 1 Batch_Num 157 Loss 0.01343335397541523\n",
      "Epoch 1 Batch_Num 158 Loss 0.007131983991712332\n",
      "Epoch 1 Batch_Num 159 Loss 0.010089699178934097\n",
      "Epoch 1 Batch_Num 160 Loss 0.012536699883639812\n",
      "Epoch 1 Batch_Num 161 Loss 0.026768933981657028\n",
      "Epoch 1 Batch_Num 162 Loss 0.012560246512293816\n",
      "Epoch 1 Batch_Num 163 Loss 0.015236297622323036\n",
      "Epoch 1 Batch_Num 164 Loss 0.013831788673996925\n",
      "Epoch 1 Batch_Num 165 Loss 0.009940103627741337\n",
      "Epoch 1 Batch_Num 166 Loss 0.017501424998044968\n",
      "Epoch 1 Batch_Num 167 Loss 0.016939058899879456\n",
      "Epoch 1 Batch_Num 168 Loss 0.013716833665966988\n",
      "Epoch 1 Batch_Num 169 Loss 0.009983658790588379\n",
      "Epoch 1 Batch_Num 170 Loss 0.015063134953379631\n",
      "Epoch 1 Batch_Num 171 Loss 0.01436150074005127\n",
      "Epoch 1 Batch_Num 172 Loss 0.020098449662327766\n",
      "Epoch 1 Batch_Num 173 Loss 0.015302503481507301\n",
      "Epoch 1 Batch_Num 174 Loss 0.00885654054582119\n",
      "Epoch 1 Batch_Num 175 Loss 0.015007819049060345\n",
      "Epoch 1 Batch_Num 176 Loss 0.010799774900078773\n",
      "Epoch 1 Batch_Num 177 Loss 0.012464922852814198\n",
      "Epoch 1 Batch_Num 178 Loss 0.017208028584718704\n",
      "Epoch 1 Batch_Num 179 Loss 0.016382792964577675\n",
      "Epoch 1 Batch_Num 180 Loss 0.019983774051070213\n",
      "Epoch 1 Batch_Num 181 Loss 0.012850631959736347\n",
      "Epoch 1 Batch_Num 182 Loss 0.012644519098103046\n",
      "Epoch 1 Batch_Num 183 Loss 0.013636855408549309\n",
      "Epoch 1 Batch_Num 184 Loss 0.01856924779713154\n",
      "Epoch 1 Batch_Num 185 Loss 0.018755462020635605\n",
      "Epoch 1 Batch_Num 186 Loss 0.01395378541201353\n",
      "Epoch 1 Batch_Num 187 Loss 0.007033903151750565\n",
      "Epoch 1 Batch_Num 188 Loss 0.012469585053622723\n",
      "Epoch 1 Batch_Num 189 Loss 0.005402437876909971\n",
      "Epoch 1 Batch_Num 190 Loss 0.004417586140334606\n",
      "Epoch 1 Batch_Num 191 Loss 0.02536693774163723\n",
      "Epoch 1 Batch_Num 192 Loss 0.01276039145886898\n",
      "Epoch 1 Batch_Num 193 Loss 0.025061916559934616\n",
      "Epoch 1 Batch_Num 194 Loss 0.014780928380787373\n",
      "Epoch 1 Batch_Num 195 Loss 0.007950526662170887\n",
      "Epoch 1 Batch_Num 196 Loss 0.018392400816082954\n",
      "Epoch 1 Batch_Num 197 Loss 0.008714227005839348\n",
      "Epoch 1 Batch_Num 198 Loss 0.024745669215917587\n",
      "Epoch 1 Batch_Num 199 Loss 0.0076916394755244255\n",
      "Epoch 1 Batch_Num 200 Loss 0.012088166549801826\n",
      "Epoch 1 Batch_Num 201 Loss 0.009412008337676525\n",
      "Epoch 1 Batch_Num 202 Loss 0.013486090116202831\n",
      "Epoch 1 Batch_Num 203 Loss 0.026421908289194107\n",
      "Epoch 1 Batch_Num 204 Loss 0.01890590786933899\n",
      "Epoch 1 Batch_Num 205 Loss 0.008802304975688457\n",
      "Epoch 1 Batch_Num 206 Loss 0.03245604783296585\n",
      "Epoch 1 Batch_Num 207 Loss 0.013817926868796349\n",
      "Epoch 1 Batch_Num 208 Loss 0.012138700112700462\n",
      "Epoch 1 Batch_Num 209 Loss 0.01813298463821411\n",
      "Epoch 1 Batch_Num 210 Loss 0.020214460790157318\n",
      "Epoch 1 Batch_Num 211 Loss 0.010813673958182335\n",
      "Epoch 1 Batch_Num 212 Loss 0.008114668540656567\n",
      "Epoch 1 Batch_Num 213 Loss 0.00896896980702877\n",
      "Epoch 1 Batch_Num 214 Loss 0.017286863178014755\n",
      "Epoch 1 Batch_Num 215 Loss 0.007029585540294647\n",
      "Epoch 1 Batch_Num 216 Loss 0.013901883736252785\n",
      "Epoch 1 Batch_Num 217 Loss 0.021602565422654152\n",
      "Epoch 1 Batch_Num 218 Loss 0.010389519855380058\n",
      "Epoch 1 Batch_Num 219 Loss 0.008813397958874702\n",
      "Epoch 1 Batch_Num 220 Loss 0.012059991247951984\n",
      "Epoch 1 Batch_Num 221 Loss 0.009894772432744503\n",
      "Epoch 1 Batch_Num 222 Loss 0.013150510378181934\n",
      "Epoch 1 Batch_Num 223 Loss 0.006732536945492029\n",
      "Epoch 1 Batch_Num 224 Loss 0.00976257212460041\n",
      "Epoch 1 Batch_Num 225 Loss 0.02254568599164486\n",
      "Epoch 1 Batch_Num 226 Loss 0.009350412525236607\n",
      "Epoch 1 Batch_Num 227 Loss 0.00041701519512571394\n",
      "Epoch 1 Batch_Num 228 Loss 0.0013774947728961706\n",
      "Epoch 1 Batch_Num 229 Loss 0.005998089909553528\n",
      "Epoch 1 Batch_Num 230 Loss 0.00033986091148108244\n",
      "Epoch 1 Batch_Num 231 Loss 0.006889823824167252\n",
      "Epoch 1 Batch_Num 232 Loss 0.003633978543803096\n",
      "Epoch 1 Batch_Num 233 Loss 0.02042143978178501\n",
      "Epoch 1 Batch_Num 234 Loss 0.02829071506857872\n",
      "Epoch 2 Batch_Num 0 Loss 0.02172592282295227\n",
      "Epoch 2 Batch_Num 1 Loss 0.015609772875905037\n",
      "Epoch 2 Batch_Num 2 Loss 0.01022701058536768\n",
      "Epoch 2 Batch_Num 3 Loss 0.018019912764430046\n",
      "Epoch 2 Batch_Num 4 Loss 0.01496011484414339\n",
      "Epoch 2 Batch_Num 5 Loss 0.01290217973291874\n",
      "Epoch 2 Batch_Num 6 Loss 0.01112242229282856\n",
      "Epoch 2 Batch_Num 7 Loss 0.007776480168104172\n",
      "Epoch 2 Batch_Num 8 Loss 0.005896307062357664\n",
      "Epoch 2 Batch_Num 9 Loss 0.010286742821335793\n",
      "Epoch 2 Batch_Num 10 Loss 0.015299046412110329\n",
      "Epoch 2 Batch_Num 11 Loss 0.012651294469833374\n",
      "Epoch 2 Batch_Num 12 Loss 0.006080309860408306\n",
      "Epoch 2 Batch_Num 13 Loss 0.008520113304257393\n",
      "Epoch 2 Batch_Num 14 Loss 0.01427287794649601\n",
      "Epoch 2 Batch_Num 15 Loss 0.008862810209393501\n",
      "Epoch 2 Batch_Num 16 Loss 0.00809420831501484\n",
      "Epoch 2 Batch_Num 17 Loss 0.01131664402782917\n",
      "Epoch 2 Batch_Num 18 Loss 0.008345173671841621\n",
      "Epoch 2 Batch_Num 19 Loss 0.011820094659924507\n",
      "Epoch 2 Batch_Num 20 Loss 0.014180335216224194\n",
      "Epoch 2 Batch_Num 21 Loss 0.00833512656390667\n",
      "Epoch 2 Batch_Num 22 Loss 0.017839699983596802\n",
      "Epoch 2 Batch_Num 23 Loss 0.008231190033257008\n",
      "Epoch 2 Batch_Num 24 Loss 0.009471540339291096\n",
      "Epoch 2 Batch_Num 25 Loss 0.012463654391467571\n",
      "Epoch 2 Batch_Num 26 Loss 0.018147801980376244\n",
      "Epoch 2 Batch_Num 27 Loss 0.01372169516980648\n",
      "Epoch 2 Batch_Num 28 Loss 0.012967744842171669\n",
      "Epoch 2 Batch_Num 29 Loss 0.008943488821387291\n",
      "Epoch 2 Batch_Num 30 Loss 0.010858356952667236\n",
      "Epoch 2 Batch_Num 31 Loss 0.009528825990855694\n",
      "Epoch 2 Batch_Num 32 Loss 0.015947535634040833\n",
      "Epoch 2 Batch_Num 33 Loss 0.00911644846200943\n",
      "Epoch 2 Batch_Num 34 Loss 0.028146613389253616\n",
      "Epoch 2 Batch_Num 35 Loss 0.008517798967659473\n",
      "Epoch 2 Batch_Num 36 Loss 0.01649545505642891\n",
      "Epoch 2 Batch_Num 37 Loss 0.008632146753370762\n",
      "Epoch 2 Batch_Num 38 Loss 0.005456835962831974\n",
      "Epoch 2 Batch_Num 39 Loss 0.011370650492608547\n",
      "Epoch 2 Batch_Num 40 Loss 0.008653683587908745\n",
      "Epoch 2 Batch_Num 41 Loss 0.0032747164368629456\n",
      "Epoch 2 Batch_Num 42 Loss 0.015440842136740685\n",
      "Epoch 2 Batch_Num 43 Loss 0.004385014064610004\n",
      "Epoch 2 Batch_Num 44 Loss 0.009601058438420296\n",
      "Epoch 2 Batch_Num 45 Loss 0.022540077567100525\n",
      "Epoch 2 Batch_Num 46 Loss 0.013446837663650513\n",
      "Epoch 2 Batch_Num 47 Loss 0.00884644128382206\n",
      "Epoch 2 Batch_Num 48 Loss 0.012324266135692596\n",
      "Epoch 2 Batch_Num 49 Loss 0.009967471472918987\n",
      "Epoch 2 Batch_Num 50 Loss 0.012260997667908669\n",
      "Epoch 2 Batch_Num 51 Loss 0.01650654710829258\n",
      "Epoch 2 Batch_Num 52 Loss 0.01028490997850895\n",
      "Epoch 2 Batch_Num 53 Loss 0.012717770412564278\n",
      "Epoch 2 Batch_Num 54 Loss 0.011428178288042545\n",
      "Epoch 2 Batch_Num 55 Loss 0.015975669026374817\n",
      "Epoch 2 Batch_Num 56 Loss 0.01050080917775631\n",
      "Epoch 2 Batch_Num 57 Loss 0.010710244998335838\n",
      "Epoch 2 Batch_Num 58 Loss 0.004158069379627705\n",
      "Epoch 2 Batch_Num 59 Loss 0.011825231835246086\n",
      "Epoch 2 Batch_Num 60 Loss 0.008918278850615025\n",
      "Epoch 2 Batch_Num 61 Loss 0.009633143432438374\n",
      "Epoch 2 Batch_Num 62 Loss 0.01715584471821785\n",
      "Epoch 2 Batch_Num 63 Loss 0.010765554383397102\n",
      "Epoch 2 Batch_Num 64 Loss 0.007235081400722265\n",
      "Epoch 2 Batch_Num 65 Loss 0.011948281899094582\n",
      "Epoch 2 Batch_Num 66 Loss 0.0076177408918738365\n",
      "Epoch 2 Batch_Num 67 Loss 0.014034261927008629\n",
      "Epoch 2 Batch_Num 68 Loss 0.010215419344604015\n",
      "Epoch 2 Batch_Num 69 Loss 0.015539211221039295\n",
      "Epoch 2 Batch_Num 70 Loss 0.004215367138385773\n",
      "Epoch 2 Batch_Num 71 Loss 0.009270909242331982\n",
      "Epoch 2 Batch_Num 72 Loss 0.006963989231735468\n",
      "Epoch 2 Batch_Num 73 Loss 0.00748018454760313\n",
      "Epoch 2 Batch_Num 74 Loss 0.011209012940526009\n",
      "Epoch 2 Batch_Num 75 Loss 0.009396253153681755\n",
      "Epoch 2 Batch_Num 76 Loss 0.0037816266994923353\n",
      "Epoch 2 Batch_Num 77 Loss 0.0073908306658267975\n",
      "Epoch 2 Batch_Num 78 Loss 0.020422454923391342\n",
      "Epoch 2 Batch_Num 79 Loss 0.008448926731944084\n",
      "Epoch 2 Batch_Num 80 Loss 0.01764042302966118\n",
      "Epoch 2 Batch_Num 81 Loss 0.010337667539715767\n",
      "Epoch 2 Batch_Num 82 Loss 0.008411974646151066\n",
      "Epoch 2 Batch_Num 83 Loss 0.004749753046780825\n",
      "Epoch 2 Batch_Num 84 Loss 0.014140116982161999\n",
      "Epoch 2 Batch_Num 85 Loss 0.009297892451286316\n",
      "Epoch 2 Batch_Num 86 Loss 0.009586600586771965\n",
      "Epoch 2 Batch_Num 87 Loss 0.006747315172106028\n",
      "Epoch 2 Batch_Num 88 Loss 0.01255999505519867\n",
      "Epoch 2 Batch_Num 89 Loss 0.001678651780821383\n",
      "Epoch 2 Batch_Num 90 Loss 0.007564750965684652\n",
      "Epoch 2 Batch_Num 91 Loss 0.005324285011738539\n",
      "Epoch 2 Batch_Num 92 Loss 0.01665337383747101\n",
      "Epoch 2 Batch_Num 93 Loss 0.011052700690925121\n",
      "Epoch 2 Batch_Num 94 Loss 0.01030786894261837\n",
      "Epoch 2 Batch_Num 95 Loss 0.008158870972692966\n",
      "Epoch 2 Batch_Num 96 Loss 0.01881777122616768\n",
      "Epoch 2 Batch_Num 97 Loss 0.004140221513807774\n",
      "Epoch 2 Batch_Num 98 Loss 0.012665892019867897\n",
      "Epoch 2 Batch_Num 99 Loss 0.008341747336089611\n",
      "Epoch 2 Batch_Num 100 Loss 0.0060892957262694836\n",
      "Epoch 2 Batch_Num 101 Loss 0.002894763369113207\n",
      "Epoch 2 Batch_Num 102 Loss 0.003488630522042513\n",
      "Epoch 2 Batch_Num 103 Loss 0.02404559776186943\n",
      "Epoch 2 Batch_Num 104 Loss 0.016502242535352707\n",
      "Epoch 2 Batch_Num 105 Loss 0.011949347332119942\n",
      "Epoch 2 Batch_Num 106 Loss 0.012811362743377686\n",
      "Epoch 2 Batch_Num 107 Loss 0.018148843199014664\n",
      "Epoch 2 Batch_Num 108 Loss 0.010753951966762543\n",
      "Epoch 2 Batch_Num 109 Loss 0.01366051472723484\n",
      "Epoch 2 Batch_Num 110 Loss 0.014415895566344261\n",
      "Epoch 2 Batch_Num 111 Loss 0.012804409489035606\n",
      "Epoch 2 Batch_Num 112 Loss 0.011184926144778728\n",
      "Epoch 2 Batch_Num 113 Loss 0.011002796702086926\n",
      "Epoch 2 Batch_Num 114 Loss 0.013223523274064064\n",
      "Epoch 2 Batch_Num 115 Loss 0.006535379681736231\n",
      "Epoch 2 Batch_Num 116 Loss 0.010894281789660454\n",
      "Epoch 2 Batch_Num 117 Loss 0.0065398490987718105\n",
      "Epoch 2 Batch_Num 118 Loss 0.004119068384170532\n",
      "Epoch 2 Batch_Num 119 Loss 0.018820062279701233\n",
      "Epoch 2 Batch_Num 120 Loss 0.007943558506667614\n",
      "Epoch 2 Batch_Num 121 Loss 0.007621824741363525\n",
      "Epoch 2 Batch_Num 122 Loss 0.009265372529625893\n",
      "Epoch 2 Batch_Num 123 Loss 0.014742748811841011\n",
      "Epoch 2 Batch_Num 124 Loss 0.008641168475151062\n",
      "Epoch 2 Batch_Num 125 Loss 0.00612126337364316\n",
      "Epoch 2 Batch_Num 126 Loss 0.02613607607781887\n",
      "Epoch 2 Batch_Num 127 Loss 0.003984456416219473\n",
      "Epoch 2 Batch_Num 128 Loss 0.010070649906992912\n",
      "Epoch 2 Batch_Num 129 Loss 0.007428472395986319\n",
      "Epoch 2 Batch_Num 130 Loss 0.008953692391514778\n",
      "Epoch 2 Batch_Num 131 Loss 0.007783303968608379\n",
      "Epoch 2 Batch_Num 132 Loss 0.004390686750411987\n",
      "Epoch 2 Batch_Num 133 Loss 0.004973303060978651\n",
      "Epoch 2 Batch_Num 134 Loss 0.014998393133282661\n",
      "Epoch 2 Batch_Num 135 Loss 0.014516053721308708\n",
      "Epoch 2 Batch_Num 136 Loss 0.011838461272418499\n",
      "Epoch 2 Batch_Num 137 Loss 0.010049006901681423\n",
      "Epoch 2 Batch_Num 138 Loss 0.007361088879406452\n",
      "Epoch 2 Batch_Num 139 Loss 0.010934069752693176\n",
      "Epoch 2 Batch_Num 140 Loss 0.009958654642105103\n",
      "Epoch 2 Batch_Num 141 Loss 0.00899478793144226\n",
      "Epoch 2 Batch_Num 142 Loss 0.013115473091602325\n",
      "Epoch 2 Batch_Num 143 Loss 0.00907850917428732\n",
      "Epoch 2 Batch_Num 144 Loss 0.011539541184902191\n",
      "Epoch 2 Batch_Num 145 Loss 0.008127743378281593\n",
      "Epoch 2 Batch_Num 146 Loss 0.014932863414287567\n",
      "Epoch 2 Batch_Num 147 Loss 0.022664804011583328\n",
      "Epoch 2 Batch_Num 148 Loss 0.0071709780022501945\n",
      "Epoch 2 Batch_Num 149 Loss 0.014635811559855938\n",
      "Epoch 2 Batch_Num 150 Loss 0.013653610832989216\n",
      "Epoch 2 Batch_Num 151 Loss 0.010349837131798267\n",
      "Epoch 2 Batch_Num 152 Loss 0.006525865290313959\n",
      "Epoch 2 Batch_Num 153 Loss 0.01912182755768299\n",
      "Epoch 2 Batch_Num 154 Loss 0.013358376920223236\n",
      "Epoch 2 Batch_Num 155 Loss 0.009239310398697853\n",
      "Epoch 2 Batch_Num 156 Loss 0.01091664470732212\n",
      "Epoch 2 Batch_Num 157 Loss 0.0066885799169540405\n",
      "Epoch 2 Batch_Num 158 Loss 0.005894573871046305\n",
      "Epoch 2 Batch_Num 159 Loss 0.006874407641589642\n",
      "Epoch 2 Batch_Num 160 Loss 0.008893238380551338\n",
      "Epoch 2 Batch_Num 161 Loss 0.02517206035554409\n",
      "Epoch 2 Batch_Num 162 Loss 0.011636584997177124\n",
      "Epoch 2 Batch_Num 163 Loss 0.011541498824954033\n",
      "Epoch 2 Batch_Num 164 Loss 0.009238592348992825\n",
      "Epoch 2 Batch_Num 165 Loss 0.006963580846786499\n",
      "Epoch 2 Batch_Num 166 Loss 0.013633807189762592\n",
      "Epoch 2 Batch_Num 167 Loss 0.007752897683531046\n",
      "Epoch 2 Batch_Num 168 Loss 0.016026150435209274\n",
      "Epoch 2 Batch_Num 169 Loss 0.00911806058138609\n",
      "Epoch 2 Batch_Num 170 Loss 0.014271807856857777\n",
      "Epoch 2 Batch_Num 171 Loss 0.01111883856356144\n",
      "Epoch 2 Batch_Num 172 Loss 0.01504170335829258\n",
      "Epoch 2 Batch_Num 173 Loss 0.009093969129025936\n",
      "Epoch 2 Batch_Num 174 Loss 0.0050946129485964775\n",
      "Epoch 2 Batch_Num 175 Loss 0.008974215015769005\n",
      "Epoch 2 Batch_Num 176 Loss 0.007030160631984472\n",
      "Epoch 2 Batch_Num 177 Loss 0.01115167886018753\n",
      "Epoch 2 Batch_Num 178 Loss 0.011135317385196686\n",
      "Epoch 2 Batch_Num 179 Loss 0.017224665731191635\n",
      "Epoch 2 Batch_Num 180 Loss 0.014391601085662842\n",
      "Epoch 2 Batch_Num 181 Loss 0.010700416751205921\n",
      "Epoch 2 Batch_Num 182 Loss 0.008290974423289299\n",
      "Epoch 2 Batch_Num 183 Loss 0.012678067199885845\n",
      "Epoch 2 Batch_Num 184 Loss 0.014511103741824627\n",
      "Epoch 2 Batch_Num 185 Loss 0.015864316374063492\n",
      "Epoch 2 Batch_Num 186 Loss 0.011718936264514923\n",
      "Epoch 2 Batch_Num 187 Loss 0.0076963468454778194\n",
      "Epoch 2 Batch_Num 188 Loss 0.008358992636203766\n",
      "Epoch 2 Batch_Num 189 Loss 0.0060337744653224945\n",
      "Epoch 2 Batch_Num 190 Loss 0.004586625378578901\n",
      "Epoch 2 Batch_Num 191 Loss 0.0218038372695446\n",
      "Epoch 2 Batch_Num 192 Loss 0.008499084040522575\n",
      "Epoch 2 Batch_Num 193 Loss 0.017060328274965286\n",
      "Epoch 2 Batch_Num 194 Loss 0.020082298666238785\n",
      "Epoch 2 Batch_Num 195 Loss 0.007377783767879009\n",
      "Epoch 2 Batch_Num 196 Loss 0.015840347856283188\n",
      "Epoch 2 Batch_Num 197 Loss 0.003965419251471758\n",
      "Epoch 2 Batch_Num 198 Loss 0.01029499713331461\n",
      "Epoch 2 Batch_Num 199 Loss 0.006072572432458401\n",
      "Epoch 2 Batch_Num 200 Loss 0.011688432656228542\n",
      "Epoch 2 Batch_Num 201 Loss 0.009136145934462547\n",
      "Epoch 2 Batch_Num 202 Loss 0.017276879400014877\n",
      "Epoch 2 Batch_Num 203 Loss 0.014008457772433758\n",
      "Epoch 2 Batch_Num 204 Loss 0.01327296532690525\n",
      "Epoch 2 Batch_Num 205 Loss 0.0059448471292853355\n",
      "Epoch 2 Batch_Num 206 Loss 0.027556419372558594\n",
      "Epoch 2 Batch_Num 207 Loss 0.010777996852993965\n",
      "Epoch 2 Batch_Num 208 Loss 0.013147820718586445\n",
      "Epoch 2 Batch_Num 209 Loss 0.01814892329275608\n",
      "Epoch 2 Batch_Num 210 Loss 0.01528291404247284\n",
      "Epoch 2 Batch_Num 211 Loss 0.010095895268023014\n",
      "Epoch 2 Batch_Num 212 Loss 0.006165936589241028\n",
      "Epoch 2 Batch_Num 213 Loss 0.006110583432018757\n",
      "Epoch 2 Batch_Num 214 Loss 0.009101051837205887\n",
      "Epoch 2 Batch_Num 215 Loss 0.006559833884239197\n",
      "Epoch 2 Batch_Num 216 Loss 0.014250675216317177\n",
      "Epoch 2 Batch_Num 217 Loss 0.017538297921419144\n",
      "Epoch 2 Batch_Num 218 Loss 0.005874990485608578\n",
      "Epoch 2 Batch_Num 219 Loss 0.009107791818678379\n",
      "Epoch 2 Batch_Num 220 Loss 0.009228510782122612\n",
      "Epoch 2 Batch_Num 221 Loss 0.005686349235475063\n",
      "Epoch 2 Batch_Num 222 Loss 0.007982464507222176\n",
      "Epoch 2 Batch_Num 223 Loss 0.00710312882438302\n",
      "Epoch 2 Batch_Num 224 Loss 0.00703830923885107\n",
      "Epoch 2 Batch_Num 225 Loss 0.01697668805718422\n",
      "Epoch 2 Batch_Num 226 Loss 0.008338346146047115\n",
      "Epoch 2 Batch_Num 227 Loss 0.001053153770044446\n",
      "Epoch 2 Batch_Num 228 Loss 0.0021790016908198595\n",
      "Epoch 2 Batch_Num 229 Loss 0.001482662046328187\n",
      "Epoch 2 Batch_Num 230 Loss 0.00019774441898334771\n",
      "Epoch 2 Batch_Num 231 Loss 0.0026974964421242476\n",
      "Epoch 2 Batch_Num 232 Loss 0.001713428064249456\n",
      "Epoch 2 Batch_Num 233 Loss 0.01545377355068922\n",
      "Epoch 2 Batch_Num 234 Loss 0.02609415166079998\n",
      "Epoch 3 Batch_Num 0 Loss 0.018242131918668747\n",
      "Epoch 3 Batch_Num 1 Loss 0.0127121452242136\n",
      "Epoch 3 Batch_Num 2 Loss 0.006129279267042875\n",
      "Epoch 3 Batch_Num 3 Loss 0.0173944104462862\n",
      "Epoch 3 Batch_Num 4 Loss 0.011211149394512177\n",
      "Epoch 3 Batch_Num 5 Loss 0.01117476262152195\n",
      "Epoch 3 Batch_Num 6 Loss 0.008648484945297241\n",
      "Epoch 3 Batch_Num 7 Loss 0.00609603151679039\n",
      "Epoch 3 Batch_Num 8 Loss 0.005823525600135326\n",
      "Epoch 3 Batch_Num 9 Loss 0.005712970159947872\n",
      "Epoch 3 Batch_Num 10 Loss 0.0137894656509161\n",
      "Epoch 3 Batch_Num 11 Loss 0.007742004934698343\n",
      "Epoch 3 Batch_Num 12 Loss 0.004456833004951477\n",
      "Epoch 3 Batch_Num 13 Loss 0.009036611765623093\n",
      "Epoch 3 Batch_Num 14 Loss 0.010162455961108208\n",
      "Epoch 3 Batch_Num 15 Loss 0.0038276216946542263\n",
      "Epoch 3 Batch_Num 16 Loss 0.005977346561849117\n",
      "Epoch 3 Batch_Num 17 Loss 0.008536684326827526\n",
      "Epoch 3 Batch_Num 18 Loss 0.006927672773599625\n",
      "Epoch 3 Batch_Num 19 Loss 0.010628264397382736\n",
      "Epoch 3 Batch_Num 20 Loss 0.012261728756129742\n",
      "Epoch 3 Batch_Num 21 Loss 0.004298559855669737\n",
      "Epoch 3 Batch_Num 22 Loss 0.013576406054198742\n",
      "Epoch 3 Batch_Num 23 Loss 0.0062036640010774136\n",
      "Epoch 3 Batch_Num 24 Loss 0.00788189098238945\n",
      "Epoch 3 Batch_Num 25 Loss 0.014122167602181435\n",
      "Epoch 3 Batch_Num 26 Loss 0.01327553577721119\n",
      "Epoch 3 Batch_Num 27 Loss 0.013138811103999615\n",
      "Epoch 3 Batch_Num 28 Loss 0.013016635552048683\n",
      "Epoch 3 Batch_Num 29 Loss 0.00611699465662241\n",
      "Epoch 3 Batch_Num 30 Loss 0.005632905289530754\n",
      "Epoch 3 Batch_Num 31 Loss 0.0068631358444690704\n",
      "Epoch 3 Batch_Num 32 Loss 0.015147213824093342\n",
      "Epoch 3 Batch_Num 33 Loss 0.005364053416997194\n",
      "Epoch 3 Batch_Num 34 Loss 0.018457742407917976\n",
      "Epoch 3 Batch_Num 35 Loss 0.005757428705692291\n",
      "Epoch 3 Batch_Num 36 Loss 0.010094864293932915\n",
      "Epoch 3 Batch_Num 37 Loss 0.005684038158506155\n",
      "Epoch 3 Batch_Num 38 Loss 0.004307611845433712\n",
      "Epoch 3 Batch_Num 39 Loss 0.008153397589921951\n",
      "Epoch 3 Batch_Num 40 Loss 0.007238470949232578\n",
      "Epoch 3 Batch_Num 41 Loss 0.0011257696896791458\n",
      "Epoch 3 Batch_Num 42 Loss 0.01093869749456644\n",
      "Epoch 3 Batch_Num 43 Loss 0.002966487081721425\n",
      "Epoch 3 Batch_Num 44 Loss 0.003930303268134594\n",
      "Epoch 3 Batch_Num 45 Loss 0.015893498435616493\n",
      "Epoch 3 Batch_Num 46 Loss 0.012293190695345402\n",
      "Epoch 3 Batch_Num 47 Loss 0.004298302344977856\n",
      "Epoch 3 Batch_Num 48 Loss 0.009066407568752766\n",
      "Epoch 3 Batch_Num 49 Loss 0.008858460001647472\n",
      "Epoch 3 Batch_Num 50 Loss 0.011809088289737701\n",
      "Epoch 3 Batch_Num 51 Loss 0.009365360252559185\n",
      "Epoch 3 Batch_Num 52 Loss 0.0068510533310472965\n",
      "Epoch 3 Batch_Num 53 Loss 0.010923215188086033\n",
      "Epoch 3 Batch_Num 54 Loss 0.008953227661550045\n",
      "Epoch 3 Batch_Num 55 Loss 0.013511603698134422\n",
      "Epoch 3 Batch_Num 56 Loss 0.00804394856095314\n",
      "Epoch 3 Batch_Num 57 Loss 0.0067596472799777985\n",
      "Epoch 3 Batch_Num 58 Loss 0.0016358967404812574\n",
      "Epoch 3 Batch_Num 59 Loss 0.008822164498269558\n",
      "Epoch 3 Batch_Num 60 Loss 0.0052143605425953865\n",
      "Epoch 3 Batch_Num 61 Loss 0.004114850424230099\n",
      "Epoch 3 Batch_Num 62 Loss 0.012106619775295258\n",
      "Epoch 3 Batch_Num 63 Loss 0.008307056501507759\n",
      "Epoch 3 Batch_Num 64 Loss 0.007801826111972332\n",
      "Epoch 3 Batch_Num 65 Loss 0.006052662618458271\n",
      "Epoch 3 Batch_Num 66 Loss 0.005541796796023846\n",
      "Epoch 3 Batch_Num 67 Loss 0.008391916751861572\n",
      "Epoch 3 Batch_Num 68 Loss 0.004761465825140476\n",
      "Epoch 3 Batch_Num 69 Loss 0.012017393484711647\n",
      "Epoch 3 Batch_Num 70 Loss 0.009865055792033672\n",
      "Epoch 3 Batch_Num 71 Loss 0.006998990662395954\n",
      "Epoch 3 Batch_Num 72 Loss 0.0012916693231090903\n",
      "Epoch 3 Batch_Num 73 Loss 0.005489385686814785\n",
      "Epoch 3 Batch_Num 74 Loss 0.004637038800865412\n",
      "Epoch 3 Batch_Num 75 Loss 0.00935288704931736\n",
      "Epoch 3 Batch_Num 76 Loss 0.002142481505870819\n",
      "Epoch 3 Batch_Num 77 Loss 0.0029102934058755636\n",
      "Epoch 3 Batch_Num 78 Loss 0.02183210663497448\n",
      "Epoch 3 Batch_Num 79 Loss 0.007249841932207346\n",
      "Epoch 3 Batch_Num 80 Loss 0.013884049840271473\n",
      "Epoch 3 Batch_Num 81 Loss 0.003915763925760984\n",
      "Epoch 3 Batch_Num 82 Loss 0.006143372971564531\n",
      "Epoch 3 Batch_Num 83 Loss 0.0028632334433496\n",
      "Epoch 3 Batch_Num 84 Loss 0.0071740103885531425\n",
      "Epoch 3 Batch_Num 85 Loss 0.004806268494576216\n",
      "Epoch 3 Batch_Num 86 Loss 0.013183856382966042\n",
      "Epoch 3 Batch_Num 87 Loss 0.008531267754733562\n",
      "Epoch 3 Batch_Num 88 Loss 0.014358719810843468\n",
      "Epoch 3 Batch_Num 89 Loss 0.0020113163627684116\n",
      "Epoch 3 Batch_Num 90 Loss 0.004444146063178778\n",
      "Epoch 3 Batch_Num 91 Loss 0.002408422762528062\n",
      "Epoch 3 Batch_Num 92 Loss 0.011116692796349525\n",
      "Epoch 3 Batch_Num 93 Loss 0.008904976770281792\n",
      "Epoch 3 Batch_Num 94 Loss 0.009469680488109589\n",
      "Epoch 3 Batch_Num 95 Loss 0.004146186169236898\n",
      "Epoch 3 Batch_Num 96 Loss 0.014076213352382183\n",
      "Epoch 3 Batch_Num 97 Loss 0.002633576048538089\n",
      "Epoch 3 Batch_Num 98 Loss 0.00785793922841549\n",
      "Epoch 3 Batch_Num 99 Loss 0.008035908453166485\n",
      "Epoch 3 Batch_Num 100 Loss 0.0057888999581336975\n",
      "Epoch 3 Batch_Num 101 Loss 0.003816456301137805\n",
      "Epoch 3 Batch_Num 102 Loss 0.00215738033875823\n",
      "Epoch 3 Batch_Num 103 Loss 0.022892145439982414\n",
      "Epoch 3 Batch_Num 104 Loss 0.022140610963106155\n",
      "Epoch 3 Batch_Num 105 Loss 0.008930756710469723\n",
      "Epoch 3 Batch_Num 106 Loss 0.004696464631706476\n",
      "Epoch 3 Batch_Num 107 Loss 0.014101997017860413\n",
      "Epoch 3 Batch_Num 108 Loss 0.010082807391881943\n",
      "Epoch 3 Batch_Num 109 Loss 0.012780969962477684\n",
      "Epoch 3 Batch_Num 110 Loss 0.016935603693127632\n",
      "Epoch 3 Batch_Num 111 Loss 0.006681417115032673\n",
      "Epoch 3 Batch_Num 112 Loss 0.0074482448399066925\n",
      "Epoch 3 Batch_Num 113 Loss 0.006872265599668026\n",
      "Epoch 3 Batch_Num 114 Loss 0.00957370363175869\n",
      "Epoch 3 Batch_Num 115 Loss 0.006913783494383097\n",
      "Epoch 3 Batch_Num 116 Loss 0.009346166625618935\n",
      "Epoch 3 Batch_Num 117 Loss 0.006181378848850727\n",
      "Epoch 3 Batch_Num 118 Loss 0.002195996232330799\n",
      "Epoch 3 Batch_Num 119 Loss 0.010598554275929928\n",
      "Epoch 3 Batch_Num 120 Loss 0.008500961586833\n",
      "Epoch 3 Batch_Num 121 Loss 0.007642620708793402\n",
      "Epoch 3 Batch_Num 122 Loss 0.00934890378266573\n",
      "Epoch 3 Batch_Num 123 Loss 0.012533346191048622\n",
      "Epoch 3 Batch_Num 124 Loss 0.004132040776312351\n",
      "Epoch 3 Batch_Num 125 Loss 0.005456876941025257\n",
      "Epoch 3 Batch_Num 126 Loss 0.01865834929049015\n",
      "Epoch 3 Batch_Num 127 Loss 0.0024640436749905348\n",
      "Epoch 3 Batch_Num 128 Loss 0.005085259675979614\n",
      "Epoch 3 Batch_Num 129 Loss 0.0059272525832057\n",
      "Epoch 3 Batch_Num 130 Loss 0.007856013253331184\n",
      "Epoch 3 Batch_Num 131 Loss 0.003935998771339655\n",
      "Epoch 3 Batch_Num 132 Loss 0.0031615816988050938\n",
      "Epoch 3 Batch_Num 133 Loss 0.0029407383408397436\n",
      "Epoch 3 Batch_Num 134 Loss 0.008577626198530197\n",
      "Epoch 3 Batch_Num 135 Loss 0.014279207214713097\n",
      "Epoch 3 Batch_Num 136 Loss 0.010173415765166283\n",
      "Epoch 3 Batch_Num 137 Loss 0.012471280992031097\n",
      "Epoch 3 Batch_Num 138 Loss 0.005204968154430389\n",
      "Epoch 3 Batch_Num 139 Loss 0.004376780707389116\n",
      "Epoch 3 Batch_Num 140 Loss 0.0030203168280422688\n",
      "Epoch 3 Batch_Num 141 Loss 0.006866996176540852\n",
      "Epoch 3 Batch_Num 142 Loss 0.013590964488685131\n",
      "Epoch 3 Batch_Num 143 Loss 0.004441592842340469\n",
      "Epoch 3 Batch_Num 144 Loss 0.008334314450621605\n",
      "Epoch 3 Batch_Num 145 Loss 0.004672129638493061\n",
      "Epoch 3 Batch_Num 146 Loss 0.009944302029907703\n",
      "Epoch 3 Batch_Num 147 Loss 0.01600904017686844\n",
      "Epoch 3 Batch_Num 148 Loss 0.005492383148521185\n",
      "Epoch 3 Batch_Num 149 Loss 0.0076783401891589165\n",
      "Epoch 3 Batch_Num 150 Loss 0.008802334778010845\n",
      "Epoch 3 Batch_Num 151 Loss 0.007072043605148792\n",
      "Epoch 3 Batch_Num 152 Loss 0.0040623219683766365\n",
      "Epoch 3 Batch_Num 153 Loss 0.01759560965001583\n",
      "Epoch 3 Batch_Num 154 Loss 0.010045971721410751\n",
      "Epoch 3 Batch_Num 155 Loss 0.014058557339012623\n",
      "Epoch 3 Batch_Num 156 Loss 0.008013968355953693\n",
      "Epoch 3 Batch_Num 157 Loss 0.0049153403379023075\n",
      "Epoch 3 Batch_Num 158 Loss 0.0035013884771615267\n",
      "Epoch 3 Batch_Num 159 Loss 0.007604707032442093\n",
      "Epoch 3 Batch_Num 160 Loss 0.007523542735725641\n",
      "Epoch 3 Batch_Num 161 Loss 0.015399943105876446\n",
      "Epoch 3 Batch_Num 162 Loss 0.006844489369541407\n",
      "Epoch 3 Batch_Num 163 Loss 0.008926489390432835\n",
      "Epoch 3 Batch_Num 164 Loss 0.011117696762084961\n",
      "Epoch 3 Batch_Num 165 Loss 0.0061116465367376804\n",
      "Epoch 3 Batch_Num 166 Loss 0.009482159279286861\n",
      "Epoch 3 Batch_Num 167 Loss 0.004537503235042095\n",
      "Epoch 3 Batch_Num 168 Loss 0.013196374289691448\n",
      "Epoch 3 Batch_Num 169 Loss 0.006969434209167957\n",
      "Epoch 3 Batch_Num 170 Loss 0.006584964692592621\n",
      "Epoch 3 Batch_Num 171 Loss 0.013406427577137947\n",
      "Epoch 3 Batch_Num 172 Loss 0.013411459513008595\n",
      "Epoch 3 Batch_Num 173 Loss 0.005451784934848547\n",
      "Epoch 3 Batch_Num 174 Loss 0.004355008248239756\n",
      "Epoch 3 Batch_Num 175 Loss 0.006064274813979864\n",
      "Epoch 3 Batch_Num 176 Loss 0.00514948507770896\n",
      "Epoch 3 Batch_Num 177 Loss 0.010173124261200428\n",
      "Epoch 3 Batch_Num 178 Loss 0.0044888840056955814\n",
      "Epoch 3 Batch_Num 179 Loss 0.009883909486234188\n",
      "Epoch 3 Batch_Num 180 Loss 0.00821622833609581\n",
      "Epoch 3 Batch_Num 181 Loss 0.004487626254558563\n",
      "Epoch 3 Batch_Num 182 Loss 0.002596072619780898\n",
      "Epoch 3 Batch_Num 183 Loss 0.00893928948789835\n",
      "Epoch 3 Batch_Num 184 Loss 0.016222195699810982\n",
      "Epoch 3 Batch_Num 185 Loss 0.016737665981054306\n",
      "Epoch 3 Batch_Num 186 Loss 0.005413692444562912\n",
      "Epoch 3 Batch_Num 187 Loss 0.00499321473762393\n",
      "Epoch 3 Batch_Num 188 Loss 0.007382826413959265\n",
      "Epoch 3 Batch_Num 189 Loss 0.0038439847994595766\n",
      "Epoch 3 Batch_Num 190 Loss 0.002945063868537545\n",
      "Epoch 3 Batch_Num 191 Loss 0.01831609010696411\n",
      "Epoch 3 Batch_Num 192 Loss 0.005050609353929758\n",
      "Epoch 3 Batch_Num 193 Loss 0.012370919808745384\n",
      "Epoch 3 Batch_Num 194 Loss 0.01222511287778616\n",
      "Epoch 3 Batch_Num 195 Loss 0.006255574524402618\n",
      "Epoch 3 Batch_Num 196 Loss 0.015189597383141518\n",
      "Epoch 3 Batch_Num 197 Loss 0.0010942938970401883\n",
      "Epoch 3 Batch_Num 198 Loss 0.005362538620829582\n",
      "Epoch 3 Batch_Num 199 Loss 0.0021104332990944386\n",
      "Epoch 3 Batch_Num 200 Loss 0.008414318785071373\n",
      "Epoch 3 Batch_Num 201 Loss 0.005203495267778635\n",
      "Epoch 3 Batch_Num 202 Loss 0.013366661965847015\n",
      "Epoch 3 Batch_Num 203 Loss 0.01146033126860857\n",
      "Epoch 3 Batch_Num 204 Loss 0.02171667106449604\n",
      "Epoch 3 Batch_Num 205 Loss 0.004450429696589708\n",
      "Epoch 3 Batch_Num 206 Loss 0.021207157522439957\n",
      "Epoch 3 Batch_Num 207 Loss 0.012346294708549976\n",
      "Epoch 3 Batch_Num 208 Loss 0.008488161489367485\n",
      "Epoch 3 Batch_Num 209 Loss 0.00721851596608758\n",
      "Epoch 3 Batch_Num 210 Loss 0.008756034076213837\n",
      "Epoch 3 Batch_Num 211 Loss 0.009761136025190353\n",
      "Epoch 3 Batch_Num 212 Loss 0.0049858116544783115\n",
      "Epoch 3 Batch_Num 213 Loss 0.006376304663717747\n",
      "Epoch 3 Batch_Num 214 Loss 0.003395057050511241\n",
      "Epoch 3 Batch_Num 215 Loss 0.004467685706913471\n",
      "Epoch 3 Batch_Num 216 Loss 0.009275629185140133\n",
      "Epoch 3 Batch_Num 217 Loss 0.011439270339906216\n",
      "Epoch 3 Batch_Num 218 Loss 0.00567387230694294\n",
      "Epoch 3 Batch_Num 219 Loss 0.009614625945687294\n",
      "Epoch 3 Batch_Num 220 Loss 0.008515843190252781\n",
      "Epoch 3 Batch_Num 221 Loss 0.007748864591121674\n",
      "Epoch 3 Batch_Num 222 Loss 0.00763373589143157\n",
      "Epoch 3 Batch_Num 223 Loss 0.0041516730561852455\n",
      "Epoch 3 Batch_Num 224 Loss 0.008635751903057098\n",
      "Epoch 3 Batch_Num 225 Loss 0.01022771093994379\n",
      "Epoch 3 Batch_Num 226 Loss 0.005082315765321255\n",
      "Epoch 3 Batch_Num 227 Loss 0.0007399992900900543\n",
      "Epoch 3 Batch_Num 228 Loss 0.002395081566646695\n",
      "Epoch 3 Batch_Num 229 Loss 0.0024608050007373095\n",
      "Epoch 3 Batch_Num 230 Loss 0.00030016142409294844\n",
      "Epoch 3 Batch_Num 231 Loss 0.0023761901538819075\n",
      "Epoch 3 Batch_Num 232 Loss 0.001570414169691503\n",
      "Epoch 3 Batch_Num 233 Loss 0.014518802054226398\n",
      "Epoch 3 Batch_Num 234 Loss 0.019732141867280006\n",
      "Epoch 4 Batch_Num 0 Loss 0.014487773180007935\n",
      "Epoch 4 Batch_Num 1 Loss 0.008749337866902351\n",
      "Epoch 4 Batch_Num 2 Loss 0.0068159946240484715\n",
      "Epoch 4 Batch_Num 3 Loss 0.011029773391783237\n",
      "Epoch 4 Batch_Num 4 Loss 0.009250619448721409\n",
      "Epoch 4 Batch_Num 5 Loss 0.0070971352979540825\n",
      "Epoch 4 Batch_Num 6 Loss 0.00962146371603012\n",
      "Epoch 4 Batch_Num 7 Loss 0.0025466762017458677\n",
      "Epoch 4 Batch_Num 8 Loss 0.0031777501571923494\n",
      "Epoch 4 Batch_Num 9 Loss 0.004452107939869165\n",
      "Epoch 4 Batch_Num 10 Loss 0.012217534705996513\n",
      "Epoch 4 Batch_Num 11 Loss 0.009204038418829441\n",
      "Epoch 4 Batch_Num 12 Loss 0.002515896689146757\n",
      "Epoch 4 Batch_Num 13 Loss 0.009639126248657703\n",
      "Epoch 4 Batch_Num 14 Loss 0.008901137858629227\n",
      "Epoch 4 Batch_Num 15 Loss 0.00715505238622427\n",
      "Epoch 4 Batch_Num 16 Loss 0.0043897684663534164\n",
      "Epoch 4 Batch_Num 17 Loss 0.004722814075648785\n",
      "Epoch 4 Batch_Num 18 Loss 0.005583377089351416\n",
      "Epoch 4 Batch_Num 19 Loss 0.006490681320428848\n",
      "Epoch 4 Batch_Num 20 Loss 0.009165557101368904\n",
      "Epoch 4 Batch_Num 21 Loss 0.005430012010037899\n",
      "Epoch 4 Batch_Num 22 Loss 0.009085949510335922\n",
      "Epoch 4 Batch_Num 23 Loss 0.004204041324555874\n",
      "Epoch 4 Batch_Num 24 Loss 0.004435327835381031\n",
      "Epoch 4 Batch_Num 25 Loss 0.009465856477618217\n",
      "Epoch 4 Batch_Num 26 Loss 0.008204136043787003\n",
      "Epoch 4 Batch_Num 27 Loss 0.012907890602946281\n",
      "Epoch 4 Batch_Num 28 Loss 0.007394049316644669\n",
      "Epoch 4 Batch_Num 29 Loss 0.0033481393475085497\n",
      "Epoch 4 Batch_Num 30 Loss 0.006319699343293905\n",
      "Epoch 4 Batch_Num 31 Loss 0.005076014902442694\n",
      "Epoch 4 Batch_Num 32 Loss 0.0116765471175313\n",
      "Epoch 4 Batch_Num 33 Loss 0.007108198944479227\n",
      "Epoch 4 Batch_Num 34 Loss 0.012466469779610634\n",
      "Epoch 4 Batch_Num 35 Loss 0.0036536380648612976\n",
      "Epoch 4 Batch_Num 36 Loss 0.00608085235580802\n",
      "Epoch 4 Batch_Num 37 Loss 0.006707680877298117\n",
      "Epoch 4 Batch_Num 38 Loss 0.00507764657959342\n",
      "Epoch 4 Batch_Num 39 Loss 0.0054361820220947266\n",
      "Epoch 4 Batch_Num 40 Loss 0.005807545036077499\n",
      "Epoch 4 Batch_Num 41 Loss 0.0010158795630559325\n",
      "Epoch 4 Batch_Num 42 Loss 0.009848888032138348\n",
      "Epoch 4 Batch_Num 43 Loss 0.0026900307275354862\n",
      "Epoch 4 Batch_Num 44 Loss 0.004017186816781759\n",
      "Epoch 4 Batch_Num 45 Loss 0.010052729398012161\n",
      "Epoch 4 Batch_Num 46 Loss 0.013032397255301476\n",
      "Epoch 4 Batch_Num 47 Loss 0.005643436219543219\n",
      "Epoch 4 Batch_Num 48 Loss 0.004693144001066685\n",
      "Epoch 4 Batch_Num 49 Loss 0.006966432090848684\n",
      "Epoch 4 Batch_Num 50 Loss 0.01500452309846878\n",
      "Epoch 4 Batch_Num 51 Loss 0.007892648689448833\n",
      "Epoch 4 Batch_Num 52 Loss 0.00947146862745285\n",
      "Epoch 4 Batch_Num 53 Loss 0.00551628228276968\n",
      "Epoch 4 Batch_Num 54 Loss 0.00705215847119689\n",
      "Epoch 4 Batch_Num 55 Loss 0.010825092904269695\n",
      "Epoch 4 Batch_Num 56 Loss 0.01102118194103241\n",
      "Epoch 4 Batch_Num 57 Loss 0.008088590577244759\n",
      "Epoch 4 Batch_Num 58 Loss 0.002752589527517557\n",
      "Epoch 4 Batch_Num 59 Loss 0.0036559421569108963\n",
      "Epoch 4 Batch_Num 60 Loss 0.003902466967701912\n",
      "Epoch 4 Batch_Num 61 Loss 0.006003053393214941\n",
      "Epoch 4 Batch_Num 62 Loss 0.0095506701618433\n",
      "Epoch 4 Batch_Num 63 Loss 0.0032920739613473415\n",
      "Epoch 4 Batch_Num 64 Loss 0.006140200421214104\n",
      "Epoch 4 Batch_Num 65 Loss 0.0053201159462332726\n",
      "Epoch 4 Batch_Num 66 Loss 0.002628327812999487\n",
      "Epoch 4 Batch_Num 67 Loss 0.009489720687270164\n",
      "Epoch 4 Batch_Num 68 Loss 0.003180063096806407\n",
      "Epoch 4 Batch_Num 69 Loss 0.01057946402579546\n",
      "Epoch 4 Batch_Num 70 Loss 0.006528975907713175\n",
      "Epoch 4 Batch_Num 71 Loss 0.0037684007547795773\n",
      "Epoch 4 Batch_Num 72 Loss 0.0015343260020017624\n",
      "Epoch 4 Batch_Num 73 Loss 0.002578502520918846\n",
      "Epoch 4 Batch_Num 74 Loss 0.003783389227464795\n",
      "Epoch 4 Batch_Num 75 Loss 0.003841012716293335\n",
      "Epoch 4 Batch_Num 76 Loss 0.003473738906905055\n",
      "Epoch 4 Batch_Num 77 Loss 0.004459159914404154\n",
      "Epoch 4 Batch_Num 78 Loss 0.014484410174190998\n",
      "Epoch 4 Batch_Num 79 Loss 0.006062890402972698\n",
      "Epoch 4 Batch_Num 80 Loss 0.01014681439846754\n",
      "Epoch 4 Batch_Num 81 Loss 0.002268918789923191\n",
      "Epoch 4 Batch_Num 82 Loss 0.0036924208980053663\n",
      "Epoch 4 Batch_Num 83 Loss 0.0009856813121587038\n",
      "Epoch 4 Batch_Num 84 Loss 0.005030793137848377\n",
      "Epoch 4 Batch_Num 85 Loss 0.00482240179553628\n",
      "Epoch 4 Batch_Num 86 Loss 0.0027589374221861362\n",
      "Epoch 4 Batch_Num 87 Loss 0.004706076346337795\n",
      "Epoch 4 Batch_Num 88 Loss 0.0076176137663424015\n",
      "Epoch 4 Batch_Num 89 Loss 0.0013206175062805414\n",
      "Epoch 4 Batch_Num 90 Loss 0.002400235738605261\n",
      "Epoch 4 Batch_Num 91 Loss 0.0012659572530537844\n",
      "Epoch 4 Batch_Num 92 Loss 0.009546113200485706\n",
      "Epoch 4 Batch_Num 93 Loss 0.008459237404167652\n",
      "Epoch 4 Batch_Num 94 Loss 0.0032084411941468716\n",
      "Epoch 4 Batch_Num 95 Loss 0.004721739329397678\n",
      "Epoch 4 Batch_Num 96 Loss 0.011379116214811802\n",
      "Epoch 4 Batch_Num 97 Loss 0.002927463036030531\n",
      "Epoch 4 Batch_Num 98 Loss 0.0030378149822354317\n",
      "Epoch 4 Batch_Num 99 Loss 0.0065748607739806175\n",
      "Epoch 4 Batch_Num 100 Loss 0.0054065994918346405\n",
      "Epoch 4 Batch_Num 101 Loss 0.000792992184869945\n",
      "Epoch 4 Batch_Num 102 Loss 0.001685067662037909\n",
      "Epoch 4 Batch_Num 103 Loss 0.016800109297037125\n",
      "Epoch 4 Batch_Num 104 Loss 0.020906710997223854\n",
      "Epoch 4 Batch_Num 105 Loss 0.005593118257820606\n",
      "Epoch 4 Batch_Num 106 Loss 0.007384224329143763\n",
      "Epoch 4 Batch_Num 107 Loss 0.010395552963018417\n",
      "Epoch 4 Batch_Num 108 Loss 0.01131009217351675\n",
      "Epoch 4 Batch_Num 109 Loss 0.007625262252986431\n",
      "Epoch 4 Batch_Num 110 Loss 0.006762611214071512\n",
      "Epoch 4 Batch_Num 111 Loss 0.005257279612123966\n",
      "Epoch 4 Batch_Num 112 Loss 0.004909321200102568\n",
      "Epoch 4 Batch_Num 113 Loss 0.006639587227255106\n",
      "Epoch 4 Batch_Num 114 Loss 0.005864405073225498\n",
      "Epoch 4 Batch_Num 115 Loss 0.0039794486947357655\n",
      "Epoch 4 Batch_Num 116 Loss 0.008377425372600555\n",
      "Epoch 4 Batch_Num 117 Loss 0.007052634842693806\n",
      "Epoch 4 Batch_Num 118 Loss 0.0012417540419846773\n",
      "Epoch 4 Batch_Num 119 Loss 0.006174522917717695\n",
      "Epoch 4 Batch_Num 120 Loss 0.008232085965573788\n",
      "Epoch 4 Batch_Num 121 Loss 0.004468572326004505\n",
      "Epoch 4 Batch_Num 122 Loss 0.0063693588599562645\n",
      "Epoch 4 Batch_Num 123 Loss 0.008948122151196003\n",
      "Epoch 4 Batch_Num 124 Loss 0.00196289224550128\n",
      "Epoch 4 Batch_Num 125 Loss 0.004764166660606861\n",
      "Epoch 4 Batch_Num 126 Loss 0.007771107368171215\n",
      "Epoch 4 Batch_Num 127 Loss 0.007465933915227652\n",
      "Epoch 4 Batch_Num 128 Loss 0.008115304633975029\n",
      "Epoch 4 Batch_Num 129 Loss 0.005400479771196842\n",
      "Epoch 4 Batch_Num 130 Loss 0.01005187351256609\n",
      "Epoch 4 Batch_Num 131 Loss 0.0032073291949927807\n",
      "Epoch 4 Batch_Num 132 Loss 0.0019654168281704187\n",
      "Epoch 4 Batch_Num 133 Loss 0.0015171054983511567\n",
      "Epoch 4 Batch_Num 134 Loss 0.014747753739356995\n",
      "Epoch 4 Batch_Num 135 Loss 0.011162223294377327\n",
      "Epoch 4 Batch_Num 136 Loss 0.0068219811655581\n",
      "Epoch 4 Batch_Num 137 Loss 0.010081546381115913\n",
      "Epoch 4 Batch_Num 138 Loss 0.004918000195175409\n",
      "Epoch 4 Batch_Num 139 Loss 0.004656449891626835\n",
      "Epoch 4 Batch_Num 140 Loss 0.005282493308186531\n",
      "Epoch 4 Batch_Num 141 Loss 0.004496093839406967\n",
      "Epoch 4 Batch_Num 142 Loss 0.00947011448442936\n",
      "Epoch 4 Batch_Num 143 Loss 0.006419692188501358\n",
      "Epoch 4 Batch_Num 144 Loss 0.009309974499046803\n",
      "Epoch 4 Batch_Num 145 Loss 0.006503617856651545\n",
      "Epoch 4 Batch_Num 146 Loss 0.007683198899030685\n",
      "Epoch 4 Batch_Num 147 Loss 0.008068758994340897\n",
      "Epoch 4 Batch_Num 148 Loss 0.0018676683539524674\n",
      "Epoch 4 Batch_Num 149 Loss 0.0064857155084609985\n",
      "Epoch 4 Batch_Num 150 Loss 0.007732017431408167\n",
      "Epoch 4 Batch_Num 151 Loss 0.009253198280930519\n",
      "Epoch 4 Batch_Num 152 Loss 0.002800679299980402\n",
      "Epoch 4 Batch_Num 153 Loss 0.010674824006855488\n",
      "Epoch 4 Batch_Num 154 Loss 0.006820722483098507\n",
      "Epoch 4 Batch_Num 155 Loss 0.012326514348387718\n",
      "Epoch 4 Batch_Num 156 Loss 0.011212894693017006\n",
      "Epoch 4 Batch_Num 157 Loss 0.005532999522984028\n",
      "Epoch 4 Batch_Num 158 Loss 0.0058072530664503574\n",
      "Epoch 4 Batch_Num 159 Loss 0.006302558816969395\n",
      "Epoch 4 Batch_Num 160 Loss 0.006758538074791431\n",
      "Epoch 4 Batch_Num 161 Loss 0.011118324473500252\n",
      "Epoch 4 Batch_Num 162 Loss 0.008580967783927917\n",
      "Epoch 4 Batch_Num 163 Loss 0.005920263938605785\n",
      "Epoch 4 Batch_Num 164 Loss 0.007756587117910385\n",
      "Epoch 4 Batch_Num 165 Loss 0.003996975254267454\n",
      "Epoch 4 Batch_Num 166 Loss 0.008936362341046333\n",
      "Epoch 4 Batch_Num 167 Loss 0.00423968443647027\n",
      "Epoch 4 Batch_Num 168 Loss 0.01808735355734825\n",
      "Epoch 4 Batch_Num 169 Loss 0.0073032258078455925\n",
      "Epoch 4 Batch_Num 170 Loss 0.012356352992355824\n",
      "Epoch 4 Batch_Num 171 Loss 0.0076203057542443275\n",
      "Epoch 4 Batch_Num 172 Loss 0.0036675152368843555\n",
      "Epoch 4 Batch_Num 173 Loss 0.005423938389867544\n",
      "Epoch 4 Batch_Num 174 Loss 0.001736609498038888\n",
      "Epoch 4 Batch_Num 175 Loss 0.0053665223531425\n",
      "Epoch 4 Batch_Num 176 Loss 0.0031058588065207005\n",
      "Epoch 4 Batch_Num 177 Loss 0.00449200626462698\n",
      "Epoch 4 Batch_Num 178 Loss 0.004763257224112749\n",
      "Epoch 4 Batch_Num 179 Loss 0.010624334216117859\n",
      "Epoch 4 Batch_Num 180 Loss 0.00594670232385397\n",
      "Epoch 4 Batch_Num 181 Loss 0.007539723999798298\n",
      "Epoch 4 Batch_Num 182 Loss 0.005165661685168743\n",
      "Epoch 4 Batch_Num 183 Loss 0.007580308709293604\n",
      "Epoch 4 Batch_Num 184 Loss 0.011219894513487816\n",
      "Epoch 4 Batch_Num 185 Loss 0.011966988444328308\n",
      "Epoch 4 Batch_Num 186 Loss 0.008028143085539341\n",
      "Epoch 4 Batch_Num 187 Loss 0.006101101636886597\n",
      "Epoch 4 Batch_Num 188 Loss 0.008126913569867611\n",
      "Epoch 4 Batch_Num 189 Loss 0.0011713476851582527\n",
      "Epoch 4 Batch_Num 190 Loss 0.003686482086777687\n",
      "Epoch 4 Batch_Num 191 Loss 0.012366880662739277\n",
      "Epoch 4 Batch_Num 192 Loss 0.004111909307539463\n",
      "Epoch 4 Batch_Num 193 Loss 0.009121624752879143\n",
      "Epoch 4 Batch_Num 194 Loss 0.007533597759902477\n",
      "Epoch 4 Batch_Num 195 Loss 0.006483470089733601\n",
      "Epoch 4 Batch_Num 196 Loss 0.009194809012115002\n",
      "Epoch 4 Batch_Num 197 Loss 0.0020147133618593216\n",
      "Epoch 4 Batch_Num 198 Loss 0.007746819406747818\n",
      "Epoch 4 Batch_Num 199 Loss 0.0015223721275106072\n",
      "Epoch 4 Batch_Num 200 Loss 0.012736422941088676\n",
      "Epoch 4 Batch_Num 201 Loss 0.0034352601505815983\n",
      "Epoch 4 Batch_Num 202 Loss 0.009651601314544678\n",
      "Epoch 4 Batch_Num 203 Loss 0.008773395791649818\n",
      "Epoch 4 Batch_Num 204 Loss 0.010631388053297997\n",
      "Epoch 4 Batch_Num 205 Loss 0.0037158839404582977\n",
      "Epoch 4 Batch_Num 206 Loss 0.0157428290694952\n",
      "Epoch 4 Batch_Num 207 Loss 0.007340239826589823\n",
      "Epoch 4 Batch_Num 208 Loss 0.007408567704260349\n",
      "Epoch 4 Batch_Num 209 Loss 0.0043042809702456\n",
      "Epoch 4 Batch_Num 210 Loss 0.005984376184642315\n",
      "Epoch 4 Batch_Num 211 Loss 0.00845095794647932\n",
      "Epoch 4 Batch_Num 212 Loss 0.0035225101746618748\n",
      "Epoch 4 Batch_Num 213 Loss 0.005708205047994852\n",
      "Epoch 4 Batch_Num 214 Loss 0.003178489627316594\n",
      "Epoch 4 Batch_Num 215 Loss 0.004929179325699806\n",
      "Epoch 4 Batch_Num 216 Loss 0.00952836126089096\n",
      "Epoch 4 Batch_Num 217 Loss 0.00785914622247219\n",
      "Epoch 4 Batch_Num 218 Loss 0.0045488933101296425\n",
      "Epoch 4 Batch_Num 219 Loss 0.006769442465156317\n",
      "Epoch 4 Batch_Num 220 Loss 0.009315939620137215\n",
      "Epoch 4 Batch_Num 221 Loss 0.006025048904120922\n",
      "Epoch 4 Batch_Num 222 Loss 0.005022455006837845\n",
      "Epoch 4 Batch_Num 223 Loss 0.0020996402017772198\n",
      "Epoch 4 Batch_Num 224 Loss 0.00727153429761529\n",
      "Epoch 4 Batch_Num 225 Loss 0.0077788131311535835\n",
      "Epoch 4 Batch_Num 226 Loss 0.005953615996986628\n",
      "Epoch 4 Batch_Num 227 Loss 0.00028003673651255667\n",
      "Epoch 4 Batch_Num 228 Loss 0.001162588712759316\n",
      "Epoch 4 Batch_Num 229 Loss 0.0031433571130037308\n",
      "Epoch 4 Batch_Num 230 Loss 0.0005729269469156861\n",
      "Epoch 4 Batch_Num 231 Loss 0.001149380812421441\n",
      "Epoch 4 Batch_Num 232 Loss 0.005222003906965256\n",
      "Epoch 4 Batch_Num 233 Loss 0.011856709606945515\n",
      "Epoch 4 Batch_Num 234 Loss 0.015886791050434113\n"
     ]
    }
   ],
   "source": [
    "# define a training loop \n",
    "num_epochs = 5\n",
    "for f in range(num_epochs):\n",
    "    for batch_num, minibatch in enumerate(train_loader):\n",
    "        minibatch_x, minibatch_y = minibatch[0], minibatch[1]\n",
    "\n",
    "        output = model.forward(torch.Tensor(minibatch_x.float()))\n",
    "        loss = criterion(output, torch.Tensor(minibatch_y.float()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {f} Batch_Num {batch_num} Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc:  0.9758\n",
      "auc_score:  0.986748232709577\n"
     ]
    }
   ],
   "source": [
    "# start an MLFlow run and log everything \n",
    "mlflow.set_experiment(\"PyTorch_MNIST\")\n",
    "\n",
    "with mlflow.start_run(): \n",
    "    preds = model.forward(torch.Tensor(x_test.float()))\n",
    "    preds = np.round(preds.detach().cpu().numpy()) # there's no need to detach & move to cpu\n",
    "                                            #   since i'm using cpu \n",
    "\n",
    "    eval_acc = accuracy_score(y_test, preds)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"auc_score\", auc_score)\n",
    "\n",
    "    print(\"eval_acc: \", eval_acc)\n",
    "    print(\"auc_score: \", auc_score)\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"PyTorch_MNIST\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an MLFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pytorch.load_model(\"runs:/47c97f305d1045e7a4949b406f593514/PyTorch_MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc:  0.9758\n",
      "auc_score:  0.986748232709577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\cc_env\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# make predictions and calculate metrics\n",
    "preds = loaded_model.forward(torch.Tensor(x_test.float()))\n",
    "preds = np.round(preds.detach().cpu().numpy())\n",
    "eval_acc = accuracy_score(y_test, preds)\n",
    "auc_score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(\"eval_acc: \", eval_acc)\n",
    "print(\"auc_score: \", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* output of calculating the evaluation metrics from earlier but with logged model. \n",
    "* the `scores match`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
